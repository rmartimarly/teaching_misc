{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmartimarly/teaching_misc/blob/main/AIML_13_convolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tAAMAbNz2RTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNNs\n",
        "\n",
        "Advanced Machine Learning Module (AML)\n",
        "\n",
        "Robert Marti (robert.marti@udg.edu)\n",
        "University of Girona\n",
        "\n",
        "Example from Fastbook \n",
        "https://github.com/fastai/fastbook\n"
      ],
      "metadata": {
        "id": "8DzWFyYr2SGH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Hl7_fFUytHnw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd921936-7b4e-4c74-c605-6cf17c5bb949"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#hide\n",
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "c_h6opkBtHn3"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "\n",
        "matplotlib.rc('image', cmap='Greys')"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "3CdMX5DktHn5"
      },
      "source": [
        "[[chapter_convolutions]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0bHtC2UtHn6"
      },
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH7VDkG6tHn-"
      },
      "source": [
        "In <<chapter_mnist_basics>> we learned how to create a neural network recognizing images. We were able to achieve a bit over 98% accuracy at distinguishing 3s from 7s—but we also saw that fastai's built-in classes were able to get close to 100%. Let's start trying to close the gap.\n",
        "\n",
        "In this chapter, we will begin by digging into what convolutions are and building a CNN from scratch. We will then study a range of techniques to improve training stability and learn all the tweaks the library usually applies for us to get great results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPhx-xVVtHoA"
      },
      "source": [
        "## The Magic of Convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHVDksVGtHoA"
      },
      "source": [
        "One of the most powerful tools that machine learning practitioners have at their disposal is *feature engineering*. A *feature* is a transformation of the data which is designed to make it easier to model. For instance, the `add_datepart` function that we used for our tabular dataset preprocessing in <<chapter_tabular>> added date features to the Bulldozers dataset. What kinds of features might we be able to create from images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHZOp0AgtHoB"
      },
      "source": [
        "> jargon: Feature engineering: Creating new transformations of the input data in order to make it easier to model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1PFJAT-tHoC"
      },
      "source": [
        "In the context of an image, a feature is a visually distinctive attribute. For example, the number 7 is characterized by a horizontal edge near the top of the digit, and a top-right to bottom-left diagonal edge underneath that. On the other hand, the number 3 is characterized by a diagonal edge in one direction at the top left and bottom right of the digit, the opposite diagonal at the bottom left and top right, horizontal edges at the middle, top, and bottom, and so forth. So what if we could extract information about where the edges occur in each image, and then use that information as our features, instead of raw pixels?\n",
        "\n",
        "It turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use something called a *convolution*. A convolution requires nothing more than multiplication, and addition—two operations that are responsible for the vast majority of work that we will see in every single deep learning model in this book!\n",
        "\n",
        "A convolution applies a *kernel* across an image. A kernel is a little matrix, such as the 3×3 matrix in the top right of <<basic_conv>>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxtpZHmCtHoD"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter9_conv_basic.png?raw=1\" id=\"basic_conv\" caption=\"Applying a kernel to one location\" alt=\"Applying a kernel to one location\" width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh36SUbktHoE"
      },
      "source": [
        "The 7×7 grid to the left is the *image* we're going to apply the kernel to. The convolution operation multiplies each element of the kernel by each element of a 3×3 block of the image. The results of these multiplications are then added together. The diagram in <<basic_conv>> shows an example of applying a kernel to a single location in the image, the 3×3 block around cell 18.\n",
        "\n",
        "Let's do this with code. First, we create a little 3×3 matrix like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Hu9RvVvJtHoF"
      },
      "outputs": [],
      "source": [
        "top_edge = tensor([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]]).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0bf7qShtHoG"
      },
      "source": [
        "We're going to call this our kernel (because that's what fancy computer vision researchers call these). And we'll need an image, of course:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "h7RZYPOmtHoG"
      },
      "outputs": [],
      "source": [
        "path = untar_data(URLs.MNIST_SAMPLE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(path)\n",
        "!ls /root/.fastai/data/mnist_sample\n",
        "!ls /root/.fastai/data/mnist_sample/train/3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVkE0DWq39pR",
        "outputId": "4967a603-0b87-4803-dcf5-984844ee13d2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/.fastai/data/mnist_sample\n",
            "labels.csv  train  valid\n",
            "10000.png  17470.png  25210.png  33283.png  41136.png  48932.png  5640.png\n",
            "10011.png  17471.png  25222.png  33299.png  41143.png  48935.png  56420.png\n",
            "10031.png  17491.png  25225.png  33316.png  41165.png  48965.png  5642.png\n",
            "10034.png  17496.png  25231.png  3332.png   41171.png  48974.png  56434.png\n",
            "10042.png  17509.png  25245.png  33331.png  41212.png  48978.png  56437.png\n",
            "10052.png  17521.png  2524.png\t 33344.png  4121.png   48981.png  56448.png\n",
            "10074.png  17526.png  25257.png  33351.png  41225.png  4898.png   56449.png\n",
            "1007.png   17553.png  25266.png  33357.png  41229.png  48995.png  56457.png\n",
            "10091.png  17569.png  25282.png  3336.png   41233.png  48998.png  56474.png\n",
            "10093.png  1756.png   25284.png  33371.png  41267.png  49006.png  56475.png\n",
            "10097.png  17574.png  25291.png  33399.png  41283.png  49018.png  56488.png\n",
            "10099.png  17575.png  25296.png  33400.png  41287.png  49020.png  56492.png\n",
            "10116.png  17580.png  2529.png\t 33407.png  41299.png  49054.png  56495.png\n",
            "10125.png  17582.png  25311.png  33412.png  41317.png  49055.png  5649.png\n",
            "10137.png  17588.png  2531.png\t 33421.png  41337.png  49057.png  56504.png\n",
            "10141.png  17593.png  25328.png  33432.png  41343.png  49077.png  56515.png\n",
            "10144.png  17621.png  25331.png  33444.png  41356.png  49080.png  56524.png\n",
            "10155.png  17624.png  25346.png  33445.png  4136.png   49081.png  56528.png\n",
            "10161.png  17629.png  2535.png\t 33468.png  41378.png  49082.png  56535.png\n",
            "10206.png  17632.png  2536.png\t 33471.png  41385.png  49091.png  56561.png\n",
            "10210.png  17634.png  25373.png  3347.png   4138.png   490.png\t  56565.png\n",
            "10214.png  17643.png  25377.png  33491.png  41391.png  49103.png  56574.png\n",
            "1021.png   17646.png  25398.png  33500.png  41404.png  49108.png  56578.png\n",
            "10238.png  17673.png  25409.png  33507.png  41407.png  49117.png  56581.png\n",
            "10260.png  17676.png  25413.png  33510.png  41411.png  49122.png  56585.png\n",
            "10278.png  17699.png  25417.png  33529.png  41428.png  49135.png  5660.png\n",
            "10282.png  17710.png  2541.png\t 33532.png  41436.png  49137.png  56615.png\n",
            "10314.png  17712.png  25425.png  33566.png  41453.png  49142.png  56616.png\n",
            "10322.png  17715.png  25431.png  33568.png  41471.png  49147.png  56621.png\n",
            "10328.png  17731.png  25453.png  33583.png  41476.png  49149.png  56624.png\n",
            "10329.png  17734.png  25454.png  33587.png  41491.png  49156.png  56645.png\n",
            "10330.png  17745.png  25480.png  33589.png  41494.png  49157.png  56654.png\n",
            "10349.png  17752.png  254.png\t 3358.png   41507.png  49166.png  56657.png\n",
            "1035.png   17753.png  25509.png  33590.png  41513.png  4917.png   56697.png\n",
            "10360.png  17754.png  25523.png  33594.png  41515.png  49183.png  56721.png\n",
            "10369.png  17766.png  25535.png  33607.png  41537.png  49188.png  56730.png\n",
            "10389.png  17777.png  25536.png  33609.png  41543.png  49193.png  56738.png\n",
            "10390.png  17778.png  25538.png  33624.png  4154.png   49207.png  56741.png\n",
            "10393.png  17781.png  25552.png  33633.png  41552.png  49222.png  56754.png\n",
            "10402.png  17787.png  25553.png  33649.png  41565.png  49228.png  56761.png\n",
            "10405.png  1778.png   25573.png  33655.png  4156.png   49230.png  56776.png\n",
            "10415.png  17790.png  25586.png  33694.png  41573.png  49234.png  56791.png\n",
            "10423.png  17810.png  25590.png  33709.png  41590.png  49249.png  5679.png\n",
            "10424.png  17820.png  255.png\t 33722.png  41608.png  49254.png  56817.png\n",
            "10430.png  17832.png  25601.png  33729.png  41619.png  49269.png  56820.png\n",
            "10450.png  17853.png  25619.png  33745.png  41624.png  49286.png  56823.png\n",
            "10453.png  17864.png  25630.png  33754.png  41629.png  49287.png  5683.png\n",
            "10454.png  17893.png  25631.png  33758.png  41632.png  49288.png  56849.png\n",
            "10466.png  17901.png  25641.png  33760.png  41639.png  49294.png  56869.png\n",
            "10470.png  17904.png  25645.png  33763.png  41653.png  49297.png  56871.png\n",
            "10483.png  17906.png  2564.png\t 33772.png  41689.png  49300.png  56875.png\n",
            "10493.png  17908.png  25671.png  33786.png  41704.png  49307.png  5687.png\n",
            "10511.png  17911.png  2567.png\t 33795.png  41720.png  49312.png  56921.png\n",
            "10518.png  1791.png   25685.png  33817.png  41727.png  49318.png  56925.png\n",
            "10520.png  17929.png  25688.png  33818.png  41740.png  49321.png  56927.png\n",
            "10527.png  17947.png  25689.png  33828.png  41742.png  49324.png  56934.png\n",
            "1055.png   17966.png  2568.png\t 33837.png  41747.png  49329.png  56966.png\n",
            "10569.png  17967.png  25695.png  3383.png   41753.png  49350.png  56974.png\n",
            "10585.png  17988.png  25708.png  33840.png  41755.png  49376.png  56975.png\n",
            "10605.png  17993.png  25721.png  33845.png  41765.png  49378.png  56994.png\n",
            "10606.png  17997.png  25728.png  33854.png  41767.png  4937.png   56995.png\n",
            "10608.png  1799.png   25740.png  3386.png   41773.png  49390.png  57004.png\n",
            "10635.png  179.png    25746.png  33875.png  41776.png  49404.png  57009.png\n",
            "10644.png  18001.png  25768.png  33877.png  41781.png  49406.png  57016.png\n",
            "10647.png  18004.png  25789.png  33880.png  4179.png   49410.png  5702.png\n",
            "10666.png  18007.png  25802.png  33896.png  41820.png  49431.png  57040.png\n",
            "10668.png  18030.png  25809.png  33908.png  41822.png  49433.png  57056.png\n",
            "10674.png  18039.png  2580.png\t 33911.png  41834.png  49435.png  57103.png\n",
            "10676.png  18044.png  25820.png  33917.png  41843.png  49446.png  57107.png\n",
            "10677.png  18049.png  25829.png  33939.png  41850.png  49450.png  5710.png\n",
            "10678.png  18059.png  25855.png  33953.png  41863.png  49461.png  57131.png\n",
            "1068.png   1805.png   25861.png  33962.png  41883.png  49467.png  57141.png\n",
            "10696.png  18061.png  25867.png  33966.png  41902.png  49477.png  57149.png\n",
            "10728.png  18069.png  2587.png\t 33969.png  41917.png  49494.png  57166.png\n",
            "10730.png  18079.png  25880.png  33970.png  4191.png   49529.png  57171.png\n",
            "10734.png  18099.png  25882.png  3397.png   41929.png  49556.png  57172.png\n",
            "10743.png  1809.png   25883.png  33986.png  41943.png  49560.png  57173.png\n",
            "10745.png  18108.png  25896.png  33989.png  41960.png  49566.png  5717.png\n",
            "10756.png  18119.png  25911.png  34006.png  41968.png  4956.png   57194.png\n",
            "10757.png  18120.png  25915.png  3400.png   41969.png  49570.png  57195.png\n",
            "10760.png  18155.png  25922.png  34018.png  4196.png   4957.png   57199.png\n",
            "10762.png  18169.png  25924.png  34023.png  41978.png  49585.png  57207.png\n",
            "10765.png  181.png    25941.png  34026.png  41994.png  49586.png  57227.png\n",
            "10775.png  18218.png  25949.png  3402.png   4199.png   49589.png  57247.png\n",
            "10779.png  18225.png  25950.png  34048.png  42024.png  495.png\t  57264.png\n",
            "1077.png   18227.png  25955.png  3404.png   42041.png  49602.png  57265.png\n",
            "10787.png  18240.png  25993.png  34058.png  42043.png  49607.png  57278.png\n",
            "10796.png  18243.png  25996.png  34076.png  42054.png  49627.png  5728.png\n",
            "107.png    18244.png  26008.png  34085.png  42055.png  49638.png  57291.png\n",
            "10807.png  18249.png  26013.png  34089.png  42056.png  49645.png  57300.png\n",
            "10826.png  18252.png  26014.png  34092.png  42070.png  49648.png  57306.png\n",
            "10827.png  18268.png  26033.png  34105.png  42073.png  49650.png  57308.png\n",
            "10830.png  18274.png  2603.png\t 34119.png  42082.png  49654.png  57312.png\n",
            "10847.png  18280.png  26048.png  34120.png  42083.png  4966.png   57336.png\n",
            "10865.png  18285.png  26050.png  34127.png  42087.png  49670.png  57341.png\n",
            "10872.png  18287.png  26053.png  34140.png  42088.png  49673.png  57351.png\n",
            "10876.png  18299.png  26057.png  34143.png  42097.png  49678.png  5736.png\n",
            "10880.png  1829.png   26081.png  34145.png  42114.png  49725.png  57373.png\n",
            "10882.png  18307.png  26084.png  3415.png   42117.png  49729.png  5738.png\n",
            "10887.png  18325.png  26123.png  34170.png  42120.png  49732.png  57391.png\n",
            "10896.png  18343.png  26127.png  34171.png  42137.png  49746.png  57396.png\n",
            "10900.png  18363.png  26130.png  34175.png  42140.png  49748.png  57399.png\n",
            "10902.png  1836.png   26140.png  34180.png  42150.png  49749.png  57401.png\n",
            "10921.png  18375.png  26148.png  34183.png  42161.png  49763.png  57402.png\n",
            "10953.png  18380.png  26149.png  34188.png  42178.png  49771.png  57415.png\n",
            "10966.png  18390.png  26154.png  34190.png  42186.png  49789.png  57424.png\n",
            "1097.png   18397.png  26156.png  341.png    42199.png  49791.png  57426.png\n",
            "10984.png  18426.png  26165.png  34203.png  42201.png  49809.png  57441.png\n",
            "10993.png  1843.png   26171.png  34204.png  42203.png  49810.png  57457.png\n",
            "10994.png  18453.png  26187.png  34208.png  42210.png  49813.png  57468.png\n",
            "10998.png  18463.png  26194.png  34219.png  42212.png  49814.png  57472.png\n",
            "10.png\t   18477.png  26208.png  3422.png   42215.png  49821.png  57474.png\n",
            "11002.png  18485.png  26209.png  34235.png  42220.png  49826.png  57477.png\n",
            "11010.png  18503.png  26222.png  34239.png  42235.png  49841.png  57488.png\n",
            "11011.png  18522.png  26247.png  3424.png   4223.png   49859.png  5748.png\n",
            "11012.png  18527.png  26267.png  34259.png  42248.png  49864.png  57492.png\n",
            "11021.png  18528.png  26270.png  34268.png  42251.png  49868.png  57495.png\n",
            "11031.png  18542.png  2627.png\t 34289.png  42254.png  49882.png  574.png\n",
            "11034.png  18549.png  26284.png  34293.png  42255.png  4988.png   57522.png\n",
            "11037.png  18569.png  26291.png  34315.png  42257.png  49893.png  57523.png\n",
            "11039.png  18584.png  26305.png  34316.png  42264.png  49896.png  57527.png\n",
            "11042.png  18587.png  26306.png  34318.png  42284.png  49912.png  57533.png\n",
            "11053.png  18588.png  26307.png  34327.png  42303.png  49913.png  5753.png\n",
            "11075.png  18612.png  26311.png  34333.png  4233.png   49926.png  57568.png\n",
            "11083.png  18615.png  26324.png  34351.png  42343.png  49938.png  57584.png\n",
            "1108.png   18626.png  2632.png\t 3435.png   4234.png   49946.png  57594.png\n",
            "11097.png  18627.png  26358.png  34363.png  42355.png  49948.png  57598.png\n",
            "11104.png  1862.png   26364.png  34364.png  4235.png   49955.png  57608.png\n",
            "11111.png  18630.png  26378.png  34366.png  4236.png   49966.png  57612.png\n",
            "11114.png  1863.png   26383.png  34368.png  42375.png  49968.png  57625.png\n",
            "11138.png  18646.png  26391.png  34384.png  4238.png   4996.png   57635.png\n",
            "11155.png  18675.png  26399.png  34387.png  42391.png  49981.png  57651.png\n",
            "11169.png  18679.png  26403.png  34390.png  42398.png  49985.png  57660.png\n",
            "11171.png  18687.png  26412.png  34396.png  42419.png  49.png\t  57679.png\n",
            "11197.png  1870.png   26419.png  34407.png  42423.png  50000.png  57680.png\n",
            "1119.png   18727.png  26433.png  34427.png  42432.png  50007.png  57689.png\n",
            "111.png    1872.png   26448.png  34461.png  42434.png  50012.png  57698.png\n",
            "11201.png  18735.png  26449.png  34481.png  42441.png  5001.png   5770.png\n",
            "11216.png  18739.png  26463.png  34525.png  42445.png  50025.png  57717.png\n",
            "11219.png  18742.png  26478.png  34526.png  42469.png  50033.png  57734.png\n",
            "1121.png   18752.png  26488.png  3452.png   4246.png   50043.png  57735.png\n",
            "11221.png  18765.png  26503.png  34536.png  42476.png  50045.png  57739.png\n",
            "11232.png  18766.png  26505.png  34547.png  42477.png  50067.png  57750.png\n",
            "11233.png  18770.png  26526.png  34548.png  42493.png  50087.png  57762.png\n",
            "11234.png  18785.png  2652.png\t 34551.png  42505.png  500.png\t  57765.png\n",
            "11244.png  18796.png  26534.png  34552.png  42508.png  50107.png  57766.png\n",
            "11259.png  18802.png  26542.png  34556.png  4250.png   5011.png   57771.png\n",
            "11284.png  18805.png  26548.png  34587.png  42526.png  50131.png  57781.png\n",
            "11285.png  18812.png  26550.png  34597.png  42534.png  50140.png  57788.png\n",
            "11296.png  18814.png  26554.png  34605.png  42545.png  50141.png  57801.png\n",
            "11301.png  18826.png  26555.png  34607.png  42565.png  50148.png  57804.png\n",
            "1130.png   18827.png  26561.png  3460.png   4256.png   50175.png  57806.png\n",
            "11321.png  1883.png   26593.png  34622.png  4257.png   50181.png  57817.png\n",
            "11341.png  18840.png  26596.png  34629.png  42585.png  50184.png  57820.png\n",
            "11377.png  18847.png  2660.png\t 3462.png   42588.png  50195.png  57833.png\n",
            "11392.png  18862.png  2661.png\t 34630.png  425.png    50201.png  57856.png\n",
            "11441.png  18895.png  26625.png  34642.png  42605.png  50206.png  57863.png\n",
            "1144.png   18915.png  26629.png  34644.png  42627.png  50210.png  57864.png\n",
            "11454.png  18919.png  26644.png  34645.png  4262.png   50212.png  57868.png\n",
            "1145.png   18920.png  26651.png  34646.png  42636.png  50213.png  57872.png\n",
            "11467.png  18924.png  26662.png  34659.png  4263.png   50214.png  57877.png\n",
            "11476.png  18939.png  26671.png  34660.png  42669.png  50253.png  57879.png\n",
            "11480.png  18948.png  26674.png  3466.png   42677.png  50260.png  5787.png\n",
            "11481.png  18955.png  26678.png  34672.png  42678.png  50262.png  57886.png\n",
            "11489.png  18960.png  26691.png  34684.png  42681.png  50265.png  5788.png\n",
            "1150.png   18964.png  26692.png  34685.png  42707.png  50286.png  57893.png\n",
            "11511.png  18965.png  26694.png  34695.png  4271.png   50288.png  57897.png\n",
            "11513.png  18974.png  2669.png\t 34701.png  42735.png  50297.png  57909.png\n",
            "1151.png   18975.png  26702.png  34710.png  42755.png  50300.png  57914.png\n",
            "11520.png  18977.png  26710.png  34712.png  42758.png  50308.png  57919.png\n",
            "11524.png  18986.png  26711.png  34713.png  4276.png   50312.png  57920.png\n",
            "11535.png  18992.png  26715.png  34716.png  42775.png  50317.png  57932.png\n",
            "11539.png  18997.png  2671.png\t 3471.png   42777.png  5031.png   5793.png\n",
            "11544.png  19014.png  26724.png  34721.png  42791.png  50330.png  57941.png\n",
            "11545.png  19015.png  26732.png  34728.png  42794.png  50336.png  57952.png\n",
            "11554.png  19016.png  26739.png  3472.png   42804.png  50337.png  57967.png\n",
            "11556.png  19063.png  26744.png  34736.png  42807.png  50340.png  57974.png\n",
            "11565.png  19075.png  2675.png\t 34740.png  4281.png   50361.png  57983.png\n",
            "11570.png  1908.png   26781.png  34788.png  42823.png  50364.png  5798.png\n",
            "11583.png  19092.png  26783.png  34793.png  42840.png  50369.png  57990.png\n",
            "1158.png   19101.png  26809.png  34802.png  42841.png  5036.png   5799.png\n",
            "11618.png  19110.png  26824.png  34805.png  42886.png  50372.png  58001.png\n",
            "11619.png  19114.png  26825.png  34815.png  42898.png  50374.png  58021.png\n",
            "11645.png  19116.png  26830.png  34828.png  42902.png  50378.png  58045.png\n",
            "11651.png  19169.png  26831.png  34832.png  42906.png  50401.png  58048.png\n",
            "11665.png  19181.png  26849.png  34845.png  42907.png  50403.png  58064.png\n",
            "11693.png  19195.png  26866.png  34846.png  42908.png  50412.png  58065.png\n",
            "11705.png  19200.png  26871.png  34853.png  42909.png  50417.png  58067.png\n",
            "11708.png  19205.png  26878.png  34862.png  42914.png  5041.png   58077.png\n",
            "11710.png  19215.png  26880.png  34866.png  42930.png  50431.png  58079.png\n",
            "11714.png  1921.png   26909.png  34873.png  42931.png  50470.png  58080.png\n",
            "11719.png  19222.png  26928.png  34893.png  42938.png  50481.png  58084.png\n",
            "11747.png  19230.png  26929.png  34898.png  42951.png  50491.png  58086.png\n",
            "11752.png  19231.png  26942.png  34906.png  42954.png  50500.png  58089.png\n",
            "11755.png  19234.png  26943.png  3491.png   4295.png   50516.png  58097.png\n",
            "11756.png  19238.png  26965.png  34931.png  42962.png  50529.png  58113.png\n",
            "11777.png  19246.png  26975.png  34967.png  42966.png  50545.png  5812.png\n",
            "11781.png  19247.png  26981.png  34973.png  42971.png  50554.png  58144.png\n",
            "11788.png  1924.png   26989.png  34987.png  42994.png  50560.png  58149.png\n",
            "1178.png   19250.png  26998.png  34995.png  43012.png  50565.png  58163.png\n",
            "11797.png  19263.png  27022.png  35010.png  43014.png  50603.png  58173.png\n",
            "11804.png  19264.png  27031.png  35015.png  43027.png  50611.png  58183.png\n",
            "11813.png  19266.png  27032.png  35022.png  43029.png  50613.png  581.png\n",
            "11837.png  19280.png  27047.png  35030.png  43031.png  50644.png  58200.png\n",
            "1185.png   19285.png  2705.png\t 35043.png  43043.png  50658.png  5820.png\n",
            "11863.png  19302.png  27060.png  35045.png  43064.png  50671.png  58219.png\n",
            "11874.png  19307.png  27067.png  35049.png  43065.png  50672.png  58227.png\n",
            "11910.png  19326.png  27075.png  35058.png  43067.png  50679.png  58237.png\n",
            "11921.png  19333.png  27078.png  35066.png  43070.png  5067.png   58249.png\n",
            "11929.png  19335.png  2707.png\t 35073.png  43076.png  50680.png  58250.png\n",
            "11949.png  19337.png  27084.png  35086.png  43080.png  50683.png  58252.png\n",
            "11951.png  19338.png  27095.png  35093.png  43090.png  50688.png  58257.png\n",
            "11958.png  19342.png  27115.png  35113.png  43100.png  50693.png  58268.png\n",
            "11960.png  19347.png  27132.png  35122.png  43103.png  50713.png  58278.png\n",
            "11978.png  19348.png  27133.png  35132.png  43126.png  50717.png  58288.png\n",
            "11979.png  1935.png   27135.png  35139.png  43132.png  50726.png  58298.png\n",
            "11999.png  19368.png  27140.png  35143.png  43135.png  50728.png  58311.png\n",
            "12012.png  19377.png  27143.png  35155.png  43140.png  50735.png  5831.png\n",
            "12017.png  19386.png  27152.png  35156.png  43155.png  50755.png  58324.png\n",
            "1201.png   1938.png   27160.png  35162.png  43171.png  50770.png  58327.png\n",
            "12022.png  19413.png  27167.png  35165.png  43180.png  50775.png  58328.png\n",
            "12024.png  1941.png   27172.png  35170.png  43192.png  50803.png  58333.png\n",
            "12035.png  19425.png  27173.png  35173.png  43196.png  50809.png  58336.png\n",
            "12044.png  19438.png  27183.png  35180.png  43198.png  5080.png   58339.png\n",
            "12063.png  19442.png  2720.png\t 35190.png  43200.png  50815.png  58343.png\n",
            "12080.png  19445.png  27211.png  35195.png  43204.png  50824.png  58379.png\n",
            "12086.png  19465.png  27212.png  35208.png  43208.png  50834.png  58389.png\n",
            "12089.png  19474.png  27214.png  3521.png   43209.png  50837.png  5838.png\n",
            "12094.png  19495.png  27238.png  35221.png  43224.png  5086.png   58399.png\n",
            "12100.png  19500.png  27248.png  35226.png  43230.png  50885.png  58406.png\n",
            "12108.png  19501.png  27261.png  35231.png  43235.png  5088.png   58410.png\n",
            "12109.png  19518.png  2726.png\t 3523.png   43252.png  50892.png  58411.png\n",
            "12117.png  19536.png  27274.png  35248.png  43255.png  50906.png  58412.png\n",
            "12124.png  19548.png  27279.png  35249.png  43256.png  50915.png  58421.png\n",
            "12148.png  19549.png  27282.png  35271.png  43280.png  50922.png  58431.png\n",
            "12151.png  19575.png  27287.png  35272.png  43287.png  5092.png   58449.png\n",
            "12152.png  19579.png  27313.png  35317.png  4328.png   50935.png  58465.png\n",
            "12153.png  1959.png   27316.png  35333.png  43298.png  50936.png  58483.png\n",
            "12183.png  19601.png  27333.png  35336.png  43301.png  50940.png  58491.png\n",
            "12186.png  19615.png  27349.png  35346.png  43327.png  50946.png  58495.png\n",
            "12189.png  19620.png  2734.png\t 35349.png  43330.png  50950.png  58503.png\n",
            "1218.png   19631.png  2735.png\t 35354.png  43353.png  50955.png  58513.png\n",
            "12211.png  19633.png  27362.png  35366.png  4335.png   50960.png  58525.png\n",
            "12214.png  19655.png  27366.png  35376.png  43373.png  50964.png  58527.png\n",
            "12218.png  19660.png  27389.png  35379.png  43380.png  50969.png  5852.png\n",
            "12222.png  19675.png  2740.png\t 35381.png  43393.png  50975.png  58541.png\n",
            "12225.png  19695.png  27419.png  35401.png  4339.png   50976.png  58544.png\n",
            "12228.png  19708.png  27420.png  35407.png  433.png    5097.png   58550.png\n",
            "12244.png  19713.png  27423.png  35415.png  43417.png  50982.png  58558.png\n",
            "12245.png  1971.png   27435.png  35422.png  43428.png  50989.png  58564.png\n",
            "12246.png  19734.png  27443.png  3542.png   4342.png   50995.png  58582.png\n",
            "12260.png  19736.png  27467.png  35434.png  43435.png  509.png\t  58587.png\n",
            "12265.png  19739.png  27478.png  35447.png  43438.png  50.png\t  5858.png\n",
            "1226.png   19758.png  27480.png  35458.png  43448.png  5102.png   58602.png\n",
            "12274.png  19793.png  27486.png  35459.png  43456.png  51030.png  58612.png\n",
            "12300.png  19803.png  27518.png  35486.png  43460.png  51038.png  58620.png\n",
            "12303.png  1981.png   2751.png\t 35491.png  43466.png  51054.png  58640.png\n",
            "12307.png  19821.png  27527.png  35498.png  43474.png  51063.png  58642.png\n",
            "12312.png  19829.png  27534.png  35504.png  43475.png  51085.png  58654.png\n",
            "12326.png  19834.png  27540.png  3550.png   43476.png  51091.png  58656.png\n",
            "12329.png  19836.png  27541.png  35511.png  43482.png  51092.png  58660.png\n",
            "12330.png  19855.png  27553.png  35520.png  43492.png  51101.png  58664.png\n",
            "12333.png  19863.png  27560.png  35522.png  43494.png  51106.png  58667.png\n",
            "12334.png  19865.png  27570.png  35528.png  43509.png  51116.png  58678.png\n",
            "12338.png  19866.png  27573.png  3552.png   43517.png  51133.png  58700.png\n",
            "1233.png   19878.png  27589.png  35530.png  4352.png   51142.png  58707.png\n",
            "12344.png  19883.png  2760.png\t 35531.png  43537.png  51153.png  5870.png\n",
            "12345.png  198.png    27610.png  35542.png  43552.png  51154.png  58711.png\n",
            "12346.png  19905.png  27625.png  35548.png  43557.png  51173.png  5871.png\n",
            "12349.png  19921.png  27640.png  35550.png  43562.png  51196.png  58721.png\n",
            "1234.png   19941.png  27644.png  35557.png  43563.png  51201.png  58729.png\n",
            "12350.png  1994.png   27645.png  35562.png  43573.png  51211.png  58747.png\n",
            "12355.png  19950.png  27649.png  35563.png  43599.png  51217.png  58758.png\n",
            "12377.png  19962.png  27651.png  35568.png  4359.png   51222.png  58759.png\n",
            "12392.png  19965.png  27666.png  35574.png  43619.png  51230.png  5875.png\n",
            "12416.png  19967.png  27670.png  35582.png  43637.png  51237.png  58760.png\n",
            "1241.png   19976.png  27671.png  35610.png  43644.png  51245.png  58774.png\n",
            "12430.png  19977.png  27675.png  35613.png  43654.png  51257.png  58785.png\n",
            "12437.png  20003.png  27690.png  35615.png  43658.png  51260.png  58793.png\n",
            "12446.png  20009.png  27691.png  35622.png  43659.png  51268.png  58800.png\n",
            "1244.png   20016.png  27701.png  35631.png  43666.png  51274.png  58812.png\n",
            "12451.png  20023.png  27718.png  35640.png  43670.png  5127.png   58822.png\n",
            "12463.png  20051.png  2771.png\t 35643.png  43672.png  51321.png  58832.png\n",
            "12483.png  20072.png  27726.png  35649.png  43677.png  51332.png  58853.png\n",
            "12490.png  20080.png  27741.png  35658.png  4369.png   51333.png  58855.png\n",
            "12498.png  20096.png  2774.png\t 35688.png  43702.png  51343.png  58874.png\n",
            "12503.png  20099.png  27755.png  35692.png  43711.png  51373.png  58878.png\n",
            "12506.png  2011.png   27763.png  356.png    43717.png  51376.png  58881.png\n",
            "12527.png  20121.png  27783.png  35703.png  43730.png  51381.png  58892.png\n",
            "12536.png  20147.png  27784.png  3571.png   43739.png  51388.png  58924.png\n",
            "12544.png  20150.png  27786.png  35723.png  4373.png   5139.png   58931.png\n",
            "12545.png  20166.png  27795.png  35744.png  43747.png  51401.png  58937.png\n",
            "12546.png  20167.png  27803.png  35765.png  43758.png  51421.png  58946.png\n",
            "12583.png  20181.png  27804.png  3576.png   43759.png  51428.png  58955.png\n",
            "12592.png  20192.png  27828.png  35794.png  43762.png  5143.png   58966.png\n",
            "12593.png  20201.png  27835.png  35796.png  43773.png  51462.png  58971.png\n",
            "12602.png  20203.png  27838.png  35801.png  43775.png  51463.png  58983.png\n",
            "1260.png   20213.png  27840.png  35803.png  43782.png  51469.png  59001.png\n",
            "12617.png  2021.png   27847.png  35805.png  43783.png  51471.png  59014.png\n",
            "12625.png  20220.png  27856.png  35806.png  43816.png  51474.png  59016.png\n",
            "12645.png  20222.png  27861.png  35814.png  43817.png  51486.png  59038.png\n",
            "12656.png  2023.png   27866.png  35833.png  43833.png  5148.png   59046.png\n",
            "12661.png  20243.png  2786.png\t 35859.png  43853.png  51519.png  59054.png\n",
            "12667.png  20249.png  27870.png  35869.png  43865.png  51522.png  59062.png\n",
            "12677.png  20252.png  2788.png\t 35870.png  43868.png  5152.png   59070.png\n",
            "12698.png  20259.png  27899.png  35874.png  43869.png  51534.png  59081.png\n",
            "12700.png  20278.png  27917.png  35876.png  43875.png  51536.png  59092.png\n",
            "12702.png  20285.png  27930.png  35881.png  43883.png  5153.png   59100.png\n",
            "12709.png  2029.png   27937.png  35890.png  43889.png  51543.png  59104.png\n",
            "12720.png  20314.png  27946.png  35891.png  43902.png  51555.png  59119.png\n",
            "12729.png  20326.png  27960.png  35898.png  43922.png  51561.png  5911.png\n",
            "12747.png  20329.png  2796.png\t 35911.png  43942.png  51571.png  59120.png\n",
            "12750.png  20344.png  27977.png  35914.png  43943.png  51578.png  59137.png\n",
            "12761.png  20347.png  27987.png  3591.png   43952.png  5160.png   59151.png\n",
            "12775.png  20355.png  27988.png  35922.png  43962.png  51613.png  59158.png\n",
            "12780.png  20356.png  279.png\t 35929.png  43967.png  51616.png  59168.png\n",
            "12783.png  20362.png  27.png\t 35944.png  43978.png  51617.png  59178.png\n",
            "12812.png  20377.png  28006.png  35949.png  43991.png  51635.png  59195.png\n",
            "12814.png  20378.png  28026.png  35954.png  43997.png  51640.png  5919.png\n",
            "12826.png  20397.png  28031.png  35969.png  44004.png  51655.png  59202.png\n",
            "12828.png  203.png    28032.png  3596.png   4400.png   51675.png  59228.png\n",
            "12836.png  20400.png  28037.png  35979.png  44020.png  51686.png  59229.png\n",
            "12838.png  20413.png  2803.png\t 35984.png  44032.png  51695.png  5922.png\n",
            "12874.png  20423.png  28075.png  35986.png  44034.png  51701.png  59239.png\n",
            "12877.png  20426.png  28107.png  36005.png  44035.png  51706.png  5923.png\n",
            "12879.png  2042.png   28111.png  36018.png  4403.png   51717.png  59242.png\n",
            "12881.png  20430.png  28119.png  36021.png  44051.png  5173.png   59262.png\n",
            "12882.png  20437.png  28123.png  36023.png  44058.png  51756.png  59263.png\n",
            "12895.png  20439.png  28132.png  36032.png  44101.png  51759.png  5926.png\n",
            "12901.png  20451.png  28137.png  36046.png  44112.png  51763.png  59271.png\n",
            "1290.png   20458.png  28157.png  36048.png  44134.png  51768.png  59275.png\n",
            "12913.png  20470.png  2816.png\t 36057.png  44138.png  51771.png  59282.png\n",
            "12920.png  20492.png  28171.png  3606.png   44142.png  51776.png  59299.png\n",
            "12924.png  20499.png  28174.png  36076.png  44146.png  51790.png  59307.png\n",
            "12933.png  20500.png  2818.png\t 36078.png  44157.png  51798.png  59308.png\n",
            "12942.png  20538.png  28193.png  36082.png  44162.png  51799.png  59315.png\n",
            "12953.png  20540.png  28199.png  36093.png  44174.png  51811.png  59318.png\n",
            "1295.png   20544.png  281.png\t 3609.png   44181.png  51816.png  59322.png\n",
            "12967.png  20553.png  28204.png  36103.png  44188.png  51837.png  59328.png\n",
            "12968.png  20561.png  2820.png\t 36119.png  4418.png   51848.png  59334.png\n",
            "12981.png  20565.png  28224.png  36128.png  44191.png  5184.png   59350.png\n",
            "12983.png  20570.png  28252.png  36130.png  44192.png  51851.png  59352.png\n",
            "12990.png  20587.png  28256.png  36132.png  44193.png  5186.png   59356.png\n",
            "12995.png  20597.png  28263.png  36149.png  44211.png  51871.png  59366.png\n",
            "12998.png  20606.png  28264.png  3614.png   44216.png  51890.png  59375.png\n",
            "12.png\t   20610.png  28265.png  36154.png  44231.png  51896.png  59383.png\n",
            "13026.png  20615.png  28276.png  36169.png  44240.png  51903.png  59394.png\n",
            "13045.png  20616.png  28299.png  36180.png  44249.png  51909.png  59398.png\n",
            "13093.png  20626.png  28303.png  36189.png  44280.png  5190.png   59408.png\n",
            "13099.png  20631.png  28316.png  361.png    44282.png  51915.png  59416.png\n",
            "130.png    20649.png  28343.png  36206.png  44295.png  51918.png  5941.png\n",
            "13105.png  20664.png  28344.png  36216.png  4429.png   51934.png  5942.png\n",
            "13109.png  20670.png  2834.png\t 36227.png  44303.png  5193.png   59436.png\n",
            "13110.png  20673.png  28359.png  36242.png  44305.png  51940.png  59440.png\n",
            "13114.png  20675.png  28371.png  36247.png  44312.png  51946.png  59461.png\n",
            "13124.png  20685.png  28374.png  36248.png  44338.png  51947.png  59467.png\n",
            "13127.png  20687.png  28380.png  36262.png  44349.png  51970.png  59471.png\n",
            "1312.png   20697.png  28391.png  36263.png  44350.png  51987.png  59480.png\n",
            "13134.png  20705.png  28392.png  3626.png   44373.png  51991.png  59490.png\n",
            "13141.png  20708.png  28406.png  36287.png  44380.png  51998.png  59500.png\n",
            "13142.png  20728.png  28409.png  36295.png  44385.png  52014.png  59512.png\n",
            "13144.png  20737.png  28418.png  36298.png  44388.png  52022.png  59528.png\n",
            "13146.png  20738.png  28431.png  36301.png  44397.png  52027.png  59529.png\n",
            "1315.png   2074.png   28441.png  36324.png  44408.png  52035.png  59535.png\n",
            "13161.png  20752.png  28444.png  36327.png  44416.png  52038.png  59550.png\n",
            "1317.png   20756.png  28458.png  36329.png  44426.png  52041.png  59569.png\n",
            "13181.png  2076.png   28472.png  36337.png  44435.png  52059.png  59588.png\n",
            "13190.png  20780.png  28476.png  36338.png  44439.png  5207.png   5958.png\n",
            "13193.png  20790.png  28485.png  36357.png  44440.png  52085.png  59598.png\n",
            "13202.png  20797.png  28499.png  36377.png  44456.png  52096.png  59624.png\n",
            "13208.png  207.png    28515.png  36406.png  44457.png  52098.png  59630.png\n",
            "1320.png   20800.png  28527.png  36409.png  44462.png  52101.png  59631.png\n",
            "13212.png  20805.png  28528.png  36421.png  44468.png  52104.png  59643.png\n",
            "13218.png  20833.png  28536.png  36427.png  44477.png  52111.png  59652.png\n",
            "13221.png  20842.png  28561.png  36432.png  44478.png  52116.png  59660.png\n",
            "13222.png  20844.png  28571.png  36449.png  44497.png  52129.png  59678.png\n",
            "13225.png  20849.png  28580.png  36458.png  4449.png   52134.png  5967.png\n",
            "13228.png  20868.png  28613.png  36471.png  44504.png  52160.png  59691.png\n",
            "13239.png  20869.png  28626.png  36488.png  44510.png  52164.png  59699.png\n",
            "13242.png  20872.png  28633.png  36511.png  44519.png  52168.png  59707.png\n",
            "13250.png  2087.png   28653.png  36515.png  44522.png  52191.png  5970.png\n",
            "13285.png  2088.png   28668.png  3651.png   44540.png  52209.png  59715.png\n",
            "13288.png  20902.png  28693.png  36525.png  44543.png  52227.png  59725.png\n",
            "13293.png  20903.png  28707.png  36542.png  44545.png  52240.png  59735.png\n",
            "13294.png  20926.png  28709.png  36555.png  44546.png  52252.png  5974.png\n",
            "13298.png  2092.png   28716.png  3655.png   44555.png  52259.png  59760.png\n",
            "13317.png  20937.png  2871.png\t 36566.png  44557.png  52269.png  59769.png\n",
            "13325.png  20939.png  28734.png  3656.png   44567.png  52272.png  59777.png\n",
            "13332.png  20941.png  28735.png  36572.png  44575.png  52281.png  59778.png\n",
            "13352.png  20942.png  28752.png  36575.png  44580.png  52287.png  59798.png\n",
            "13353.png  20946.png  28757.png  36592.png  44596.png  52298.png  59808.png\n",
            "1336.png   20952.png  28759.png  36595.png  44608.png  52300.png  5980.png\n",
            "13376.png  20965.png  28760.png  36604.png  44609.png  52307.png  59815.png\n",
            "13383.png  20985.png  28773.png  36623.png  4461.png   52327.png  59818.png\n",
            "13388.png  20989.png  28778.png  36644.png  44630.png  52350.png  59825.png\n",
            "13403.png  2098.png   28796.png  36645.png  44671.png  52371.png  59828.png\n",
            "13417.png  21001.png  28803.png  36671.png  44681.png  52375.png  59860.png\n",
            "1341.png   21007.png  28820.png  36684.png  44684.png  52378.png  59870.png\n",
            "13438.png  21018.png  28825.png  36705.png  44709.png  52379.png  5987.png\n",
            "13442.png  21026.png  28826.png  36707.png  44716.png  52385.png  59883.png\n",
            "13445.png  21035.png  2883.png\t 36709.png  4471.png   52386.png  59886.png\n",
            "13448.png  21038.png  28841.png  3670.png   44729.png  5238.png   59891.png\n",
            "13478.png  21055.png  28858.png  36716.png  44749.png  52394.png  59904.png\n",
            "13488.png  21056.png  28861.png  36723.png  44764.png  52404.png  59914.png\n",
            "1348.png   21075.png  2886.png\t 36732.png  44779.png  52415.png  59921.png\n",
            "13496.png  2107.png   28881.png  36740.png  44782.png  52432.png  59930.png\n",
            "13516.png  21080.png  28900.png  3674.png   44799.png  52435.png  59955.png\n",
            "13519.png  21092.png  28913.png  36755.png  44827.png  52446.png  59957.png\n",
            "1351.png   21098.png  28916.png  36761.png  44837.png  52455.png  59961.png\n",
            "13524.png  21099.png  28936.png  36802.png  44848.png  52464.png  59964.png\n",
            "13531.png  21102.png  28953.png  36803.png  44873.png  52476.png  59978.png\n",
            "13543.png  21112.png  28955.png  36815.png  44888.png  52479.png  59980.png\n",
            "13560.png  21116.png  28957.png  36835.png  44889.png  52487.png  59996.png\n",
            "13567.png  21129.png  2895.png\t 3683.png   44903.png  5252.png   6007.png\n",
            "13568.png  21144.png  28983.png  3684.png   44906.png  52539.png  6016.png\n",
            "13576.png  21149.png  28995.png  36852.png  4490.png   52543.png  6032.png\n",
            "13577.png  2114.png   29013.png  36855.png  4491.png   52545.png  6033.png\n",
            "13579.png  21150.png  29014.png  36862.png  44959.png  52563.png  6040.png\n",
            "1357.png   21153.png  29023.png  36872.png  44969.png  52565.png  6048.png\n",
            "135.png    21154.png  29042.png  3687.png   44.png     52585.png  6057.png\n",
            "13601.png  21156.png  29045.png  36887.png  45000.png  52590.png  6059.png\n",
            "13607.png  21170.png  2905.png\t 36899.png  45005.png  52601.png  6069.png\n",
            "13615.png  21176.png  29061.png  36911.png  45008.png  52607.png  6071.png\n",
            "13628.png  21179.png  29073.png  36912.png  45015.png  52636.png  6081.png\n",
            "13629.png  21182.png  29089.png  36917.png  45031.png  52661.png  6089.png\n",
            "13647.png  21187.png  29094.png  36931.png  45043.png  52664.png  6098.png\n",
            "13656.png  21208.png  29097.png  36946.png  45046.png  52676.png  6106.png\n",
            "13658.png  21209.png  29104.png  36957.png  4506.png   52679.png  6118.png\n",
            "13660.png  21221.png  29121.png  36990.png  45079.png  52690.png  6123.png\n",
            "13670.png  21228.png  29146.png  3699.png   45088.png  52692.png  613.png\n",
            "13673.png  21238.png  29153.png  37013.png  45089.png  52697.png  6148.png\n",
            "13675.png  21250.png  29158.png  37017.png  4508.png   52700.png  6170.png\n",
            "13676.png  21261.png  2915.png\t 37021.png  45093.png  52702.png  6175.png\n",
            "136.png    21278.png  2917.png\t 37022.png  45108.png  5270.png   6182.png\n",
            "13703.png  2127.png   29181.png  37035.png  4510.png   52713.png  6185.png\n",
            "13704.png  21284.png  29182.png  37042.png  4511.png   52740.png  6213.png\n",
            "13706.png  21285.png  29185.png  37054.png  4512.png   52747.png  6216.png\n",
            "13707.png  2128.png   291.png\t 37065.png  45135.png  52751.png  6230.png\n",
            "13713.png  21305.png  29224.png  3707.png   45150.png  52753.png  6233.png\n",
            "13736.png  21306.png  29225.png  37082.png  45151.png  52771.png  6251.png\n",
            "13740.png  2130.png   29231.png  37085.png  45164.png  52773.png  6269.png\n",
            "13745.png  21320.png  29232.png  37105.png  45171.png  5277.png   6282.png\n",
            "13750.png  21325.png  29237.png  37118.png  45195.png  52785.png  6286.png\n",
            "13773.png  21328.png  29246.png  37124.png  45196.png  52790.png  6291.png\n",
            "13778.png  2132.png   29249.png  37132.png  45217.png  52794.png  6293.png\n",
            "13787.png  21341.png  29266.png  37134.png  45239.png  52810.png  6297.png\n",
            "1378.png   21359.png  29269.png  37135.png  45246.png  52816.png  6298.png\n",
            "13824.png  21370.png  2926.png\t 3713.png   45260.png  52818.png  6299.png\n",
            "13827.png  21381.png  29270.png  37161.png  45264.png  52828.png  629.png\n",
            "13831.png  21390.png  29289.png  37163.png  45266.png  5284.png   6309.png\n",
            "13836.png  21415.png  29294.png  37166.png  45271.png  52853.png  6317.png\n",
            "13839.png  21420.png  29300.png  37167.png  45272.png  52864.png  6340.png\n",
            "13859.png  21428.png  29304.png  37169.png  45273.png  52871.png  6342.png\n",
            "13879.png  21447.png  29313.png  37174.png  45275.png  52891.png  6347.png\n",
            "13894.png  21458.png  2931.png\t 37186.png  45289.png  52904.png  6350.png\n",
            "13906.png  21466.png  29352.png  37188.png  452.png    52907.png  6362.png\n",
            "13909.png  21468.png  29353.png  37197.png  45312.png  52917.png  6398.png\n",
            "13929.png  2146.png   29356.png  37204.png  45321.png  52922.png  6405.png\n",
            "13931.png  21487.png  29357.png  37205.png  45327.png  52924.png  6410.png\n",
            "13935.png  21496.png  29366.png  37209.png  45328.png  52931.png  6415.png\n",
            "13937.png  2149.png   29383.png  37223.png  45336.png  52936.png  6416.png\n",
            "13957.png  21505.png  29391.png  37229.png  45340.png  52939.png  6420.png\n",
            "13965.png  21512.png  29398.png  37244.png  45364.png  52940.png  6427.png\n",
            "13969.png  21515.png  29415.png  37250.png  45365.png  52966.png  6428.png\n",
            "13980.png  21524.png  2941.png\t 37260.png  45377.png  52972.png  6430.png\n",
            "13983.png  21525.png  29427.png  37266.png  45389.png  52974.png  643.png\n",
            "13993.png  21531.png  29458.png  37271.png  45396.png  52994.png  6440.png\n",
            "14002.png  21539.png  29459.png  37280.png  45398.png  53035.png  6447.png\n",
            "14025.png  21559.png  29464.png  37284.png  45400.png  53037.png  6450.png\n",
            "14026.png  21570.png  29476.png  37293.png  45407.png  53039.png  645.png\n",
            "14028.png  21579.png  29483.png  37304.png  45408.png  53042.png  6467.png\n",
            "14030.png  21580.png  29504.png  37309.png  45419.png  53048.png  6489.png\n",
            "14045.png  21582.png  29506.png  37314.png  45436.png  53075.png  6497.png\n",
            "14065.png  21584.png  29521.png  37332.png  4543.png   53083.png  6500.png\n",
            "14080.png  21596.png  29528.png  37355.png  45441.png  53084.png  6516.png\n",
            "14087.png  215.png    29557.png  3735.png   45468.png  53091.png  6522.png\n",
            "14099.png  21606.png  29602.png  37361.png  45473.png  53104.png  6539.png\n",
            "1409.png   21607.png  29608.png  37362.png  45474.png  53107.png  6543.png\n",
            "14107.png  21610.png  2960.png\t 37368.png  45477.png  53108.png  6545.png\n",
            "14115.png  21622.png  29610.png  37373.png  45480.png  53123.png  6559.png\n",
            "14135.png  21625.png  29613.png  37374.png  45508.png  53136.png  6561.png\n",
            "14166.png  21637.png  2961.png\t 37383.png  45509.png  53156.png  6576.png\n",
            "14177.png  21643.png  29636.png  37397.png  45517.png  53158.png  6584.png\n",
            "14184.png  21647.png  29641.png  37404.png  4552.png   5315.png   6585.png\n",
            "14200.png  21654.png  29642.png  3740.png   45545.png  53165.png  6592.png\n",
            "14205.png  21660.png  2964.png\t 37422.png  45554.png  53167.png  6594.png\n",
            "14210.png  21673.png  29650.png  37439.png  45563.png  53182.png  6595.png\n",
            "14223.png  21685.png  29653.png  37443.png  45586.png  53209.png  659.png\n",
            "14236.png  21689.png  29670.png  37454.png  45597.png  53223.png  6601.png\n",
            "14255.png  21690.png  29671.png  37464.png  45599.png  53225.png  6612.png\n",
            "14261.png  21695.png  29675.png  37468.png  45601.png  53231.png  6632.png\n",
            "14262.png  21714.png  29701.png  37470.png  45610.png  5323.png   6651.png\n",
            "14266.png  21717.png  29719.png  37478.png  45612.png  53241.png  6658.png\n",
            "14275.png  21720.png  29757.png  37503.png  45619.png  53246.png  6662.png\n",
            "14283.png  21726.png  29763.png  37517.png  45628.png  53252.png  6667.png\n",
            "14286.png  2174.png   29765.png  37524.png  45636.png  53262.png  6687.png\n",
            "1428.png   21775.png  29785.png  37540.png  45642.png  53282.png  6706.png\n",
            "14297.png  21795.png  29791.png  3754.png   45644.png  53283.png  670.png\n",
            "1429.png   2179.png   29800.png  37555.png  45655.png  53287.png  6710.png\n",
            "14303.png  2180.png   29816.png  37566.png  45656.png  5328.png   6711.png\n",
            "14320.png  21812.png  2981.png\t 37569.png  45658.png  53305.png  6719.png\n",
            "1432.png   21815.png  29823.png  3756.png   45675.png  53325.png  6739.png\n",
            "14333.png  21831.png  29844.png  37572.png  45687.png  5332.png   6750.png\n",
            "14350.png  2183.png   29875.png  37578.png  45696.png  53332.png  6754.png\n",
            "14352.png  21853.png  29882.png  37604.png  45699.png  53345.png  675.png\n",
            "14361.png  21861.png  29889.png  37622.png  45704.png  53357.png  6762.png\n",
            "14367.png  21864.png  29894.png  37625.png  45707.png  53358.png  6766.png\n",
            "1436.png   21866.png  29896.png  37626.png  4570.png   53365.png  6784.png\n",
            "14383.png  21877.png  29899.png  37627.png  45712.png  53374.png  6809.png\n",
            "14388.png  21893.png  298.png\t 37636.png  45727.png  53376.png  6825.png\n",
            "14394.png  21930.png  29913.png  37643.png  45732.png  5337.png   6826.png\n",
            "14401.png  21938.png  29936.png  37649.png  45734.png  53382.png  6827.png\n",
            "14410.png  21939.png  29958.png  37652.png  45736.png  53385.png  6844.png\n",
            "14420.png  21946.png  29962.png  37658.png  45747.png  53402.png  6855.png\n",
            "14431.png  21947.png  29969.png  37689.png  45758.png  53412.png  6856.png\n",
            "14447.png  21948.png  29972.png  37690.png  45759.png  53419.png  6858.png\n",
            "14450.png  21950.png  29991.png  37691.png  45764.png  53447.png  6863.png\n",
            "14454.png  21966.png  30000.png  37692.png  45774.png  53453.png  6892.png\n",
            "14456.png  21974.png  30002.png  37700.png  45781.png  53457.png  6894.png\n",
            "14467.png  21980.png  30011.png  37703.png  45783.png  53474.png  6898.png\n",
            "14476.png  21986.png  30040.png  37711.png  45786.png  53475.png  6904.png\n",
            "14488.png  21993.png  30041.png  37718.png  45789.png  53480.png  6906.png\n",
            "1449.png   21998.png  30048.png  37729.png  45791.png  53482.png  6912.png\n",
            "14514.png  22000.png  30057.png  37730.png  45796.png  53488.png  6920.png\n",
            "14527.png  22002.png  3005.png\t 37745.png  4579.png   53493.png  6939.png\n",
            "14529.png  22003.png  30062.png  37746.png  45801.png  53514.png  6945.png\n",
            "14533.png  2200.png   30070.png  37756.png  45809.png  53527.png  6959.png\n",
            "14535.png  22011.png  30072.png  37765.png  45814.png  53542.png  695.png\n",
            "14548.png  22014.png  30077.png  37786.png  4581.png   53545.png  6966.png\n",
            "14552.png  22024.png  30084.png  37789.png  45822.png  53547.png  7030.png\n",
            "14558.png  22029.png  30090.png  37792.png  45836.png  5354.png   7031.png\n",
            "14566.png  22032.png  30093.png  37795.png  4583.png   53554.png  7043.png\n",
            "14567.png  2203.png   30105.png  37796.png  45843.png  53558.png  7046.png\n",
            "14577.png  22049.png  30124.png  37797.png  45848.png  53560.png  7058.png\n",
            "14588.png  22058.png  30130.png  37821.png  4584.png   53566.png  7065.png\n",
            "14619.png  22069.png  30131.png  37827.png  45850.png  53570.png  7067.png\n",
            "14624.png  2207.png   30136.png  37847.png  45854.png  53583.png  7080.png\n",
            "14635.png  22092.png  3013.png\t 37848.png  45862.png  53584.png  7097.png\n",
            "14636.png  22103.png  30159.png  37855.png  45878.png  53586.png  7106.png\n",
            "14655.png  22113.png  30175.png  37860.png  45902.png  5358.png   7123.png\n",
            "14674.png  22114.png  30183.png  37886.png  45903.png  53591.png  7132.png\n",
            "14686.png  22122.png  30193.png  37889.png  45911.png  53595.png  7134.png\n",
            "14691.png  22126.png  30206.png  37890.png  45940.png  5359.png   7147.png\n",
            "14692.png  22149.png  30213.png  37896.png  45945.png  53609.png  7150.png\n",
            "1469.png   22154.png  30226.png  37903.png  45946.png  53619.png  7156.png\n",
            "14704.png  22155.png  30229.png  37908.png  45959.png  53630.png  715.png\n",
            "1470.png   22162.png  30241.png  37912.png  4595.png   53671.png  7162.png\n",
            "14736.png  22172.png  30251.png  37913.png  45970.png  53688.png  7189.png\n",
            "14738.png  22182.png  30254.png  37927.png  45979.png  53691.png  7192.png\n",
            "1474.png   22185.png  30256.png  37928.png  45983.png  53704.png  7209.png\n",
            "14761.png  22204.png  30258.png  3793.png   45990.png  5370.png   7214.png\n",
            "14765.png  22206.png  30265.png  37940.png  45995.png  53711.png  7225.png\n",
            "1476.png   22218.png  30273.png  37943.png  459.png    53712.png  7250.png\n",
            "14786.png  22219.png  30282.png  37944.png  46005.png  53716.png  7261.png\n",
            "14789.png  22231.png  30284.png  37959.png  46027.png  53722.png  7263.png\n",
            "14792.png  22234.png  30304.png  37991.png  46051.png  53724.png  7283.png\n",
            "14803.png  22238.png  30306.png  37996.png  46061.png  53740.png  7288.png\n",
            "14810.png  22251.png  3033.png\t 38037.png  46062.png  53749.png  7293.png\n",
            "14823.png  22271.png  3034.png\t 3803.png   46068.png  53755.png  7295.png\n",
            "14825.png  2228.png   30354.png  38049.png  46070.png  53757.png  731.png\n",
            "14851.png  22293.png  30362.png  38053.png  46072.png  53758.png  7322.png\n",
            "14855.png  22304.png  30375.png  38058.png  46075.png  5379.png   7332.png\n",
            "14856.png  22307.png  30377.png  38066.png  46079.png  53803.png  7336.png\n",
            "14863.png  2230.png   30379.png  38083.png  46098.png  53815.png  7339.png\n",
            "14864.png  22315.png  30386.png  38086.png  46117.png  53838.png  7367.png\n",
            "1486.png   22318.png  30398.png  38089.png  46126.png  53846.png  7383.png\n",
            "14878.png  22322.png  30400.png  38109.png  46130.png  53863.png  7387.png\n",
            "14880.png  2232.png   30411.png  3810.png   46134.png  53874.png  7396.png\n",
            "14883.png  2233.png   30417.png  38116.png  46136.png  53875.png  7397.png\n",
            "14903.png  22366.png  30423.png  38129.png  46138.png  53889.png  7412.png\n",
            "14916.png  22374.png  30431.png  3812.png   46145.png  5388.png   7415.png\n",
            "14935.png  22404.png  30445.png  38130.png  46157.png  53903.png  7422.png\n",
            "149.png    22408.png  30461.png  38145.png  4615.png   53923.png  7433.png\n",
            "15003.png  22413.png  30468.png  3815.png   46182.png  5392.png   7455.png\n",
            "15007.png  22415.png  3047.png\t 38166.png  46184.png  53938.png  7463.png\n",
            "15008.png  22417.png  30483.png  38167.png  46187.png  53943.png  7483.png\n",
            "15010.png  2241.png   30489.png  38174.png  46208.png  53954.png  7497.png\n",
            "15014.png  22443.png  3048.png\t 38180.png  46211.png  53958.png  74.png\n",
            "15022.png  22451.png  30511.png  38193.png  46221.png  53960.png  7501.png\n",
            "15031.png  22459.png  30515.png  38199.png  46228.png  5396.png   7507.png\n",
            "15045.png  22460.png  3051.png\t 38218.png  46241.png  53975.png  7521.png\n",
            "15052.png  22468.png  30540.png  38223.png  46242.png  53987.png  752.png\n",
            "15053.png  22474.png  30571.png  38238.png  46246.png  53995.png  7556.png\n",
            "15075.png  22477.png  30574.png  38239.png  46247.png  5399.png   7569.png\n",
            "15076.png  22488.png  3057.png\t 38241.png  46269.png  54001.png  7582.png\n",
            "15077.png  22492.png  30591.png  38245.png  46279.png  54008.png  7584.png\n",
            "15080.png  22493.png  30611.png  3824.png   46280.png  54013.png  7590.png\n",
            "15090.png  22496.png  30642.png  38277.png  4628.png   54014.png  7600.png\n",
            "15097.png  22522.png  30644.png  38281.png  46295.png  54016.png  760.png\n",
            "15101.png  2252.png   30645.png  38283.png  46296.png  54020.png  7613.png\n",
            "15109.png  22536.png  30650.png  38294.png  46306.png  54028.png  7618.png\n",
            "15125.png  22542.png  30662.png  38321.png  46329.png  54029.png  7625.png\n",
            "15139.png  22567.png  30673.png  38341.png  4632.png   54060.png  7626.png\n",
            "15163.png  22569.png  30675.png  38346.png  46350.png  54062.png  7634.png\n",
            "15174.png  22578.png  30679.png  3835.png   46354.png  54073.png  7635.png\n",
            "15176.png  22587.png  30681.png  38361.png  46370.png  54074.png  7655.png\n",
            "15181.png  22598.png  30692.png  38368.png  46379.png  54077.png  7660.png\n",
            "15186.png  22619.png  30717.png  38377.png  4638.png   54084.png  7666.png\n",
            "15201.png  22625.png  30720.png  38383.png  46403.png  54088.png  7672.png\n",
            "15210.png  22631.png  30729.png  38399.png  46406.png  5408.png   7675.png\n",
            "15216.png  2263.png   30733.png  38410.png  46409.png  540.png\t  7678.png\n",
            "15226.png  22643.png  30740.png  38441.png  46424.png  54107.png  767.png\n",
            "15227.png  22663.png  30747.png  38445.png  46425.png  54126.png  7692.png\n",
            "15233.png  22668.png  30754.png  3844.png   46427.png  54127.png  7698.png\n",
            "15245.png  22674.png  3075.png\t 38451.png  46442.png  5412.png   7701.png\n",
            "15246.png  22675.png  30760.png  38485.png  46449.png  54140.png  7721.png\n",
            "15250.png  22695.png  30774.png  38495.png  46454.png  54147.png  7745.png\n",
            "15251.png  22708.png  30780.png  38507.png  46465.png  54174.png  7762.png\n",
            "15261.png  22715.png  30793.png  38520.png  46480.png  54191.png  7765.png\n",
            "15281.png  22717.png  30813.png  38529.png  46485.png  54199.png  7766.png\n",
            "15292.png  22721.png  30824.png  38536.png  46500.png  54201.png  7767.png\n",
            "15297.png  22723.png  30833.png  38549.png  46506.png  54208.png  7771.png\n",
            "1529.png   22728.png  30859.png  3855.png   46523.png  54228.png  7778.png\n",
            "15322.png  22730.png  30883.png  38565.png  46525.png  54242.png  7784.png\n",
            "15332.png  22733.png  30885.png  38580.png  46532.png  54248.png  7794.png\n",
            "15335.png  22738.png  30895.png  38593.png  46534.png  54249.png  7800.png\n",
            "15336.png  22741.png  30897.png  38602.png  46548.png  54250.png  7803.png\n",
            "15339.png  2275.png   30907.png  38627.png  46549.png  54258.png  7832.png\n",
            "1533.png   22767.png  30915.png  38653.png  46555.png  54270.png  7841.png\n",
            "15342.png  22770.png  30934.png  38657.png  46563.png  54273.png  7861.png\n",
            "15346.png  22825.png  30949.png  38670.png  46566.png  54285.png  7862.png\n",
            "15350.png  22835.png  30960.png  38687.png  46570.png  54291.png  7878.png\n",
            "15372.png  22863.png  30962.png  38712.png  46588.png  54296.png  7881.png\n",
            "15375.png  2286.png   30974.png  38714.png  4658.png   54301.png  789.png\n",
            "15377.png  22883.png  30984.png  38732.png  46592.png  54304.png  7915.png\n",
            "15379.png  22894.png  30986.png  38735.png  46595.png  54322.png  7935.png\n",
            "15388.png  228.png    30.png\t 38752.png  46600.png  5432.png   7966.png\n",
            "15409.png  22903.png  31011.png  38755.png  46621.png  54339.png  7976.png\n",
            "15412.png  22906.png  31021.png  38771.png  46634.png  5433.png   7980.png\n",
            "15437.png  22943.png  31032.png  38772.png  46639.png  54343.png  7982.png\n",
            "15443.png  22946.png  31042.png  38797.png  46652.png  5434.png   7987.png\n",
            "15453.png  22957.png  31044.png  38804.png  46666.png  54357.png  7.png\n",
            "15472.png  22963.png  31049.png  38811.png  46674.png  54377.png  8012.png\n",
            "15474.png  22964.png  31065.png  38827.png  46681.png  54397.png  8013.png\n",
            "15477.png  22977.png  3108.png\t 38859.png  46689.png  54398.png  8014.png\n",
            "15482.png  22989.png  31098.png  38867.png  46692.png  54402.png  8017.png\n",
            "15486.png  22996.png  31108.png  38882.png  46702.png  54406.png  8018.png\n",
            "15496.png  22999.png  31117.png  38892.png  46709.png  54413.png  8039.png\n",
            "15497.png  23022.png  31136.png  38894.png  46729.png  5442.png   8048.png\n",
            "15511.png  23024.png  31140.png  38900.png  46736.png  54430.png  8052.png\n",
            "15519.png  23029.png  31153.png  38904.png  46743.png  54436.png  8054.png\n",
            "1551.png   23043.png  31155.png  38911.png  4674.png   54444.png  8055.png\n",
            "15533.png  23046.png  31157.png  38927.png  46803.png  54460.png  8065.png\n",
            "15534.png  23047.png  31178.png  3893.png   46807.png  54476.png  8075.png\n",
            "15535.png  23053.png  31183.png  38955.png  46815.png  54481.png  8077.png\n",
            "15540.png  23076.png  31201.png  38970.png  4681.png   54487.png  808.png\n",
            "15544.png  23101.png  31217.png  38982.png  46821.png  54493.png  8094.png\n",
            "15547.png  23116.png  31229.png  38983.png  46834.png  54499.png  8099.png\n",
            "15572.png  23119.png  31249.png  38986.png  46838.png  54516.png  8115.png\n",
            "15587.png  23130.png  3124.png\t 38989.png  46848.png  54524.png  811.png\n",
            "1559.png   23144.png  31254.png  38990.png  46853.png  54526.png  8120.png\n",
            "15606.png  23156.png  31269.png  38991.png  46854.png  54530.png  8122.png\n",
            "15630.png  23157.png  31274.png  38998.png  46856.png  54531.png  8135.png\n",
            "15639.png  2315.png   31283.png  39040.png  46857.png  54534.png  8163.png\n",
            "15647.png  23165.png  31311.png  39061.png  46882.png  54535.png  8164.png\n",
            "15651.png  23202.png  31345.png  39066.png  46899.png  54559.png  8169.png\n",
            "15656.png  23206.png  31347.png  39081.png  46900.png  5455.png   8184.png\n",
            "15667.png  23218.png  31349.png  39099.png  46902.png  54560.png  8188.png\n",
            "1566.png   23221.png  31379.png  39100.png  46908.png  54568.png  8192.png\n",
            "15681.png  23225.png  31395.png  39108.png  46911.png  54587.png  8195.png\n",
            "15701.png  23227.png  31404.png  39116.png  46918.png  5458.png   8200.png\n",
            "15717.png  23239.png  31409.png  39122.png  46923.png  54603.png  8227.png\n",
            "15743.png  23241.png  31428.png  39124.png  46932.png  54615.png  8228.png\n",
            "15747.png  23242.png  31431.png  39125.png  46937.png  54616.png  8230.png\n",
            "15760.png  23265.png  31446.png  39127.png  4694.png   54622.png  8236.png\n",
            "15762.png  23281.png  31450.png  39136.png  46953.png  54635.png  8241.png\n",
            "15764.png  23287.png  31451.png  39140.png  46971.png  54648.png  8246.png\n",
            "15769.png  23294.png  31452.png  39143.png  46979.png  54655.png  8254.png\n",
            "15773.png  23298.png  31469.png  39181.png  46981.png  54658.png  8259.png\n",
            "15774.png  23304.png  31482.png  39184.png  46983.png  54660.png  8264.png\n",
            "15785.png  23320.png  31488.png  39189.png  46986.png  5467.png   8279.png\n",
            "15793.png  23322.png  31504.png  3920.png   47007.png  54694.png  8295.png\n",
            "157.png    23337.png  3150.png\t 39215.png  47027.png  54697.png  8338.png\n",
            "15815.png  23338.png  31515.png  39222.png  47059.png  546.png\t  8350.png\n",
            "15816.png  23344.png  31538.png  39231.png  47090.png  54700.png  8360.png\n",
            "1581.png   23357.png  31544.png  39235.png  47095.png  54704.png  8381.png\n",
            "1583.png   23373.png  31548.png  3923.png   47123.png  54706.png  8395.png\n",
            "15842.png  2339.png   31559.png  39240.png  47140.png  54707.png  8405.png\n",
            "15847.png  23412.png  31562.png  39267.png  47145.png  5471.png   8406.png\n",
            "15848.png  23419.png  31564.png  39285.png  47157.png  54722.png  840.png\n",
            "15850.png  23425.png  31579.png  39290.png  47173.png  54724.png  8425.png\n",
            "15857.png  23439.png  31582.png  392.png    47192.png  54727.png  8427.png\n",
            "15869.png  23482.png  31587.png  39301.png  47195.png  54730.png  843.png\n",
            "15872.png  23483.png  31595.png  39312.png  4719.png   54735.png  8447.png\n",
            "15889.png  23488.png  31596.png  39317.png  47207.png  54736.png  8450.png\n",
            "1588.png   23490.png  3159.png\t 39325.png  47217.png  54739.png  8455.png\n",
            "15907.png  2350.png   31602.png  39327.png  47225.png  54746.png  8475.png\n",
            "15930.png  23517.png  31610.png  39329.png  47245.png  54767.png  8494.png\n",
            "15936.png  23527.png  31627.png  39349.png  47280.png  54772.png  8498.png\n",
            "15937.png  23547.png  31631.png  39359.png  47287.png  54780.png  8505.png\n",
            "15938.png  23556.png  31636.png  39366.png  47311.png  54782.png  8524.png\n",
            "15945.png  23563.png  31641.png  39384.png  47329.png  54797.png  8528.png\n",
            "15947.png  23572.png  31664.png  3938.png   47352.png  54798.png  8537.png\n",
            "1594.png   23581.png  31667.png  39397.png  47356.png  54829.png  8541.png\n",
            "15968.png  2359.png   31680.png  39412.png  4735.png   54845.png  8544.png\n",
            "15980.png  235.png    31691.png  39415.png  47361.png  54849.png  8551.png\n",
            "15986.png  23634.png  31703.png  3941.png   47367.png  54856.png  8554.png\n",
            "15990.png  23649.png  31719.png  39445.png  4736.png   54858.png  8557.png\n",
            "1599.png   23657.png  3171.png\t 39452.png  47377.png  54859.png  8568.png\n",
            "16003.png  23663.png  31720.png  39461.png  47380.png  54867.png  856.png\n",
            "16013.png  23672.png  31722.png  39469.png  47395.png  54879.png  857.png\n",
            "16019.png  23695.png  31723.png  39489.png  4740.png   54888.png  8597.png\n",
            "16029.png  23700.png  31724.png  3949.png   47411.png  54892.png  8605.png\n",
            "1603.png   23712.png  31735.png  39509.png  47424.png  54898.png  8606.png\n",
            "16071.png  23724.png  31745.png  39520.png  47426.png  54910.png  861.png\n",
            "16074.png  23730.png  31748.png  39553.png  4742.png   54915.png  8635.png\n",
            "16075.png  23731.png  3174.png\t 39565.png  47441.png  5491.png   8643.png\n",
            "16078.png  23747.png  31759.png  39566.png  47455.png  54930.png  8663.png\n",
            "16091.png  23759.png  31775.png  39591.png  47461.png  5493.png   8672.png\n",
            "16096.png  23778.png  31788.png  39595.png  47469.png  54952.png  867.png\n",
            "16111.png  23779.png  3178.png\t 39617.png  47481.png  54953.png  8684.png\n",
            "1611.png   23796.png  31791.png  39618.png  47493.png  54972.png  8693.png\n",
            "16125.png  23799.png  31805.png  39619.png  47496.png  54976.png  86.png\n",
            "16134.png  2379.png   31811.png  39641.png  47510.png  54977.png  8700.png\n",
            "16156.png  23810.png  3181.png\t 39675.png  47514.png  5497.png   8708.png\n",
            "16171.png  23812.png  31825.png  39678.png  47519.png  54983.png  8709.png\n",
            "16177.png  23819.png  31831.png  39686.png  47520.png  54996.png  8710.png\n",
            "16190.png  23820.png  31868.png  39696.png  47533.png  549.png\t  8716.png\n",
            "16191.png  23829.png  31897.png  3969.png   47539.png  55008.png  8729.png\n",
            "16193.png  23851.png  31913.png  39704.png  4755.png   55017.png  874.png\n",
            "16208.png  23860.png  31925.png  39705.png  47587.png  55028.png  8756.png\n",
            "16220.png  23866.png  3192.png\t 39706.png  47592.png  55035.png  875.png\n",
            "16229.png  23873.png  31936.png  39721.png  47594.png  55038.png  8762.png\n",
            "16239.png  23877.png  31949.png  39728.png  47602.png  55050.png  8765.png\n",
            "16243.png  23879.png  31952.png  39741.png  47605.png  55051.png  8771.png\n",
            "16249.png  23888.png  31960.png  39744.png  47618.png  55054.png  8773.png\n",
            "16260.png  23897.png  31962.png  39761.png  47620.png  55062.png  8786.png\n",
            "16291.png  23929.png  31975.png  39768.png  47625.png  55071.png  878.png\n",
            "16296.png  23939.png  31988.png  39770.png  47628.png  55091.png  8790.png\n",
            "16311.png  23955.png  31989.png  39774.png  4762.png   55093.png  8794.png\n",
            "1631.png   23962.png  32013.png  39786.png  47643.png  55103.png  8795.png\n",
            "16329.png  23977.png  32017.png  39798.png  47651.png  55104.png  8800.png\n",
            "16336.png  23986.png  3201.png\t 39804.png  47653.png  55106.png  8808.png\n",
            "16344.png  23997.png  32025.png  39814.png  47654.png  5510.png   8816.png\n",
            "1634.png   24017.png  32031.png  39820.png  47655.png  55116.png  8818.png\n",
            "16366.png  24028.png  32042.png  39836.png  47669.png  5511.png   8821.png\n",
            "16370.png  24042.png  32044.png  39851.png  47676.png  55133.png  8824.png\n",
            "16372.png  24059.png  32071.png  39852.png  47678.png  55139.png  8830.png\n",
            "16375.png  24063.png  32087.png  39855.png  47684.png  55144.png  8865.png\n",
            "16386.png  24066.png  32091.png  39871.png  47689.png  55151.png  8879.png\n",
            "16407.png  2408.png   32106.png  39884.png  47720.png  55179.png  8880.png\n",
            "16411.png  24095.png  32114.png  39885.png  47724.png  55190.png  8882.png\n",
            "16414.png  24099.png  32115.png  39900.png  47725.png  55194.png  8884.png\n",
            "16427.png  24104.png  32137.png  39904.png  47742.png  55198.png  8888.png\n",
            "16435.png  24111.png  32157.png  39907.png  47746.png  55231.png  8893.png\n",
            "1644.png   24119.png  3215.png\t 39909.png  47749.png  55249.png  8907.png\n",
            "16453.png  24125.png  32171.png  39928.png  47760.png  55257.png  890.png\n",
            "16462.png  24132.png  32176.png  39933.png  47781.png  55283.png  8923.png\n",
            "16489.png  24142.png  32192.png  39941.png  47795.png  55292.png  8936.png\n",
            "16491.png  24144.png  32193.png  3995.png   47802.png  55301.png  8941.png\n",
            "16492.png  24145.png  32194.png  39961.png  47803.png  55312.png  8946.png\n",
            "16502.png  24176.png  321.png\t 3997.png   47818.png  5531.png   8948.png\n",
            "16503.png  24196.png  32203.png  39981.png  47821.png  55321.png  8957.png\n",
            "16504.png  24204.png  32209.png  39982.png  47834.png  55334.png  8958.png\n",
            "16510.png  24205.png  3220.png\t 40005.png  47838.png  55336.png  895.png\n",
            "16512.png  24218.png  32223.png  4001.png   4783.png   55341.png  8967.png\n",
            "1651.png   24219.png  32227.png  40020.png  47841.png  55343.png  8976.png\n",
            "16529.png  24231.png  32234.png  40035.png  47872.png  55353.png  9004.png\n",
            "16530.png  2423.png   32267.png  40051.png  47876.png  55377.png  9006.png\n",
            "16549.png  24248.png  32275.png  40055.png  47881.png  55378.png  9037.png\n",
            "16556.png  24251.png  32276.png  40068.png  47895.png  55383.png  9053.png\n",
            "16564.png  24264.png  32281.png  40078.png  47901.png  55393.png  9059.png\n",
            "16569.png  24271.png  32284.png  40079.png  47913.png  55419.png  9065.png\n",
            "16584.png  24278.png  32299.png  40087.png  47925.png  5542.png   9074.png\n",
            "16587.png  24281.png  32314.png  40100.png  4792.png   55432.png  9078.png\n",
            "1661.png   24284.png  32319.png  40104.png  47930.png  55440.png  9080.png\n",
            "16620.png  24294.png  32330.png  40109.png  47939.png  55461.png  9089.png\n",
            "16626.png  24299.png  32339.png  4011.png   47946.png  55470.png  909.png\n",
            "16645.png  242.png    3233.png\t 40121.png  47950.png  55472.png  9107.png\n",
            "16649.png  24307.png  32371.png  40161.png  4795.png   55474.png  9140.png\n",
            "16657.png  2430.png   32376.png  40166.png  47970.png  5547.png   9141.png\n",
            "16663.png  24326.png  32379.png  40184.png  47975.png  55482.png  9146.png\n",
            "16676.png  24333.png  32385.png  40185.png  47979.png  55485.png  9153.png\n",
            "16678.png  24336.png  32398.png  40202.png  47992.png  55488.png  9168.png\n",
            "16689.png  24338.png  32401.png  40205.png  479.png    55509.png  9180.png\n",
            "1668.png   24350.png  32409.png  40223.png  48014.png  55527.png  9182.png\n",
            "16693.png  24354.png  32414.png  4022.png   48031.png  55547.png  9183.png\n",
            "16708.png  24356.png  32420.png  40247.png  48038.png  55548.png  9184.png\n",
            "16715.png  24358.png  32421.png  40254.png  48045.png  55550.png  9192.png\n",
            "16732.png  24369.png  32422.png  40256.png  48064.png  55566.png  9226.png\n",
            "16743.png  24373.png  32434.png  40257.png  48065.png  55579.png  9230.png\n",
            "16748.png  24386.png  32444.png  40281.png  48076.png  55580.png  9253.png\n",
            "16755.png  24400.png  32476.png  40298.png  48088.png  55592.png  9255.png\n",
            "1675.png   24407.png  32479.png  40301.png  4808.png   55628.png  9268.png\n",
            "16763.png  24414.png  32484.png  40305.png  48091.png  5562.png   9272.png\n",
            "16764.png  24417.png  32490.png  40307.png  48137.png  55640.png  9278.png\n",
            "16774.png  24435.png  32495.png  40337.png  48139.png  55649.png  9279.png\n",
            "1677.png   24442.png  32505.png  4033.png   48163.png  55665.png  9289.png\n",
            "16783.png  24446.png  32539.png  40374.png  48164.png  55677.png  9293.png\n",
            "16793.png  24448.png  32563.png  40375.png  48174.png  5567.png   9294.png\n",
            "16811.png  24451.png  32581.png  40382.png  48179.png  55688.png  9300.png\n",
            "16825.png  24452.png  32586.png  40384.png  48193.png  55694.png  9302.png\n",
            "16831.png  24469.png  32588.png  40385.png  48213.png  55698.png  9303.png\n",
            "16849.png  24481.png  3258.png\t 40401.png  48233.png  55705.png  9319.png\n",
            "16861.png  24501.png  32610.png  40404.png  4823.png   55715.png  9324.png\n",
            "16867.png  24512.png  32619.png  40427.png  48258.png  55717.png  9335.png\n",
            "16873.png  24524.png  32642.png  40447.png  48264.png  55726.png  9355.png\n",
            "16892.png  24528.png  32644.png  40452.png  48281.png  55732.png  9363.png\n",
            "1689.png   24531.png  3264.png\t 40462.png  48291.png  55737.png  9380.png\n",
            "16900.png  24532.png  32652.png  40463.png  48300.png  5573.png   9386.png\n",
            "16911.png  24539.png  32653.png  40480.png  48302.png  55743.png  9400.png\n",
            "16922.png  24560.png  32656.png  40481.png  48311.png  55746.png  9401.png\n",
            "16924.png  2456.png   32663.png  40483.png  48321.png  55751.png  9403.png\n",
            "16927.png  24573.png  32664.png  40490.png  48324.png  55758.png  9411.png\n",
            "16940.png  24575.png  32665.png  40502.png  48325.png  55771.png  9412.png\n",
            "16945.png  24584.png  32680.png  40517.png  48336.png  5578.png   9413.png\n",
            "16954.png  24597.png  32685.png  40520.png  48352.png  55791.png  9418.png\n",
            "1698.png   24603.png  32688.png  40532.png  48355.png  55796.png  9420.png\n",
            "17000.png  24610.png  32689.png  4053.png   48360.png  557.png\t  9424.png\n",
            "17010.png  24621.png  32691.png  40540.png  4836.png   55831.png  9427.png\n",
            "17026.png  24635.png  32703.png  40550.png  48385.png  55844.png  9443.png\n",
            "17037.png  2463.png   32730.png  40555.png  48386.png  55845.png  9466.png\n",
            "17038.png  24651.png  32753.png  40570.png  48392.png  55851.png  9482.png\n",
            "17040.png  24667.png  32764.png  40572.png  48396.png  55858.png  9491.png\n",
            "17041.png  24672.png  32775.png  40574.png  48415.png  55864.png  9511.png\n",
            "17062.png  24680.png  32786.png  40575.png  48426.png  55866.png  9516.png\n",
            "17063.png  2468.png   32790.png  40578.png  48431.png  55869.png  9520.png\n",
            "17077.png  24697.png  32794.png  40588.png  48435.png  55874.png  9529.png\n",
            "17082.png  2469.png   32799.png  405.png    48445.png  55876.png  953.png\n",
            "17085.png  24731.png  327.png\t 40618.png  48465.png  55881.png  9553.png\n",
            "17088.png  24733.png  32804.png  40619.png  48480.png  5588.png   9561.png\n",
            "17100.png  24735.png  32811.png  40620.png  48482.png  55893.png  9592.png\n",
            "17103.png  24747.png  32827.png  40636.png  48485.png  55914.png  9602.png\n",
            "17104.png  24748.png  32847.png  40639.png  48498.png  55923.png  9610.png\n",
            "17105.png  24763.png  32850.png  40655.png  48499.png  55926.png  9612.png\n",
            "17116.png  24770.png  32881.png  40662.png  48500.png  55932.png  9617.png\n",
            "17127.png  24772.png  32883.png  40683.png  4850.png   55937.png  9620.png\n",
            "17131.png  24780.png  32885.png  4068.png   48521.png  55941.png  9621.png\n",
            "17139.png  24783.png  32897.png  40691.png  48551.png  55947.png  9623.png\n",
            "17146.png  24784.png  32900.png  40720.png  48557.png  55960.png  9624.png\n",
            "17148.png  24787.png  32913.png  40722.png  48563.png  55962.png  9636.png\n",
            "17154.png  24789.png  3291.png\t 40734.png  48579.png  55966.png  9637.png\n",
            "17155.png  2481.png   32931.png  40739.png  48587.png  55974.png  9638.png\n",
            "17175.png  24835.png  32933.png  4073.png   48599.png  55998.png  9639.png\n",
            "17189.png  24850.png  32940.png  40743.png  48626.png  55999.png  9648.png\n",
            "1718.png   24867.png  32944.png  40745.png  48628.png  56006.png  9663.png\n",
            "17192.png  24868.png  32960.png  40752.png  48642.png  56008.png  9664.png\n",
            "17207.png  24870.png  32964.png  40763.png  4864.png   56012.png  966.png\n",
            "17208.png  24883.png  32975.png  40765.png  48662.png  56029.png  9670.png\n",
            "17227.png  24886.png  32984.png  40789.png  48666.png  56043.png  9679.png\n",
            "1722.png   24899.png  32990.png  40805.png  48667.png  56047.png  9680.png\n",
            "17242.png  2489.png   33000.png  40811.png  4866.png   56053.png  9685.png\n",
            "17253.png  24906.png  33002.png  40818.png  48682.png  5605.png   9696.png\n",
            "17255.png  24931.png  33004.png  40830.png  48683.png  56067.png  9706.png\n",
            "17257.png  24943.png  33009.png  40836.png  4868.png   56073.png  9730.png\n",
            "17258.png  24965.png  33022.png  40858.png  486.png    5611.png   9739.png\n",
            "17271.png  24969.png  33028.png  40865.png  48700.png  56134.png  9759.png\n",
            "17274.png  24987.png  33029.png  40866.png  48701.png  56136.png  975.png\n",
            "17280.png  24989.png  33049.png  40883.png  48702.png  56139.png  9766.png\n",
            "17288.png  25000.png  33054.png  40892.png  48722.png  56164.png  9778.png\n",
            "17292.png  25007.png  33080.png  40898.png  48725.png  56165.png  9779.png\n",
            "17295.png  25028.png  33099.png  40901.png  48732.png  5616.png   9786.png\n",
            "17301.png  25032.png  330.png\t 40904.png  48735.png  56191.png  9797.png\n",
            "17308.png  25033.png  33114.png  40914.png  48746.png  56197.png  9812.png\n",
            "17318.png  25034.png  33132.png  40925.png  48761.png  56198.png  9829.png\n",
            "17320.png  2503.png   33139.png  40926.png  48770.png  561.png\t  983.png\n",
            "17324.png  25040.png  33145.png  40931.png  48781.png  56204.png  9848.png\n",
            "17328.png  25055.png  33146.png  40932.png  48783.png  56207.png  9851.png\n",
            "17330.png  2506.png   3314.png\t 40940.png  48788.png  5621.png   9864.png\n",
            "17333.png  25071.png  33161.png  40947.png  4878.png   56225.png  9868.png\n",
            "17341.png  25078.png  33169.png  4094.png   48794.png  56228.png  9871.png\n",
            "17352.png  25091.png  33186.png  40962.png  48798.png  56234.png  9879.png\n",
            "17354.png  25094.png  33189.png  40996.png  48806.png  56245.png  98.png\n",
            "17355.png  250.png    3318.png\t 41003.png  48817.png  56260.png  9904.png\n",
            "17371.png  25106.png  33194.png  41007.png  4881.png   56265.png  9913.png\n",
            "1737.png   25107.png  3319.png\t 41010.png  4883.png   56275.png  9915.png\n",
            "17386.png  25116.png  33208.png  41015.png  48849.png  56288.png  9916.png\n",
            "17389.png  25127.png  3320.png\t 4101.png   48859.png  56297.png  992.png\n",
            "17403.png  25145.png  33223.png  41048.png  48868.png  56305.png  9932.png\n",
            "17404.png  25164.png  33225.png  41053.png  48873.png  56321.png  9953.png\n",
            "17409.png  25172.png  33229.png  41067.png  48901.png  56324.png  9959.png\n",
            "17421.png  25174.png  33239.png  41071.png  48904.png  56337.png  9974.png\n",
            "17435.png  25187.png  3323.png\t 41085.png  48917.png  5637.png   9977.png\n",
            "17442.png  25191.png  33240.png  41107.png  4891.png   56383.png  998.png\n",
            "17460.png  25208.png  33263.png  41110.png  48922.png  56391.png  9991.png\n",
            "17467.png  25209.png  33276.png  41123.png  48930.png  56398.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "CS94tTaUuim1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EiFCjQ_1tHoH"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "Path.BASE_PATH = path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Wa0MImO6tHoH",
        "outputId": "51390620-0a1f-4f62-8582-e5391beafa77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "im3 = Image.open(path/'train'/'3'/'3323.png')\n",
        "show_image(im3);"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YMkkV56NxGaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "outputId": "bdd5f379-1836-4666-d977-d79b24018526",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "id": "eXJJ6gRExG0N"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "im3 = Image.open(path/'train'/'3'/'12.png')\n",
        "show_image(im3);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdj9yvk9tHoI"
      },
      "source": [
        "Now we're going to take the top 3×3-pixel square of our image, and multiply each of those values by each item in our kernel. Then we'll add them up, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "yLtMnKdotHoI",
        "outputId": "978ebb7d-d3a2-4a3c-fb55-e62fa2abef92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0., -0., -0.],\n",
              "        [0., 0., 0.],\n",
              "        [0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "im3_t = tensor(im3)\n",
        "im3_t[0:3,0:3] * top_edge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "OBwt3KbRtHoJ",
        "outputId": "2b4d3631-7c8a-4878-e6cf-cbc75b9f4488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "(im3_t[0:3,0:3] * top_edge).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfzhWO11tHoJ"
      },
      "source": [
        "Not very interesting so far—all the pixels in the top-left corner are white. But let's pick a couple of more interesting spots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "kSbQTKW4tHoJ",
        "outputId": "dc90912e-76e6-4bb4-dbf1-e305623b2abd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_407c0_row0_col0, #T_407c0_row0_col1, #T_407c0_row0_col2, #T_407c0_row0_col3, #T_407c0_row0_col4, #T_407c0_row0_col5, #T_407c0_row0_col6, #T_407c0_row0_col7, #T_407c0_row0_col8, #T_407c0_row0_col9, #T_407c0_row0_col10, #T_407c0_row0_col11, #T_407c0_row0_col12, #T_407c0_row0_col13, #T_407c0_row0_col14, #T_407c0_row0_col15, #T_407c0_row0_col16, #T_407c0_row0_col17, #T_407c0_row0_col18, #T_407c0_row0_col19, #T_407c0_row1_col0, #T_407c0_row1_col1, #T_407c0_row1_col2, #T_407c0_row1_col3, #T_407c0_row1_col4, #T_407c0_row1_col5, #T_407c0_row1_col6, #T_407c0_row1_col7, #T_407c0_row1_col8, #T_407c0_row1_col9, #T_407c0_row1_col10, #T_407c0_row1_col11, #T_407c0_row1_col12, #T_407c0_row1_col13, #T_407c0_row1_col14, #T_407c0_row1_col15, #T_407c0_row1_col16, #T_407c0_row1_col17, #T_407c0_row1_col18, #T_407c0_row1_col19, #T_407c0_row2_col0, #T_407c0_row2_col1, #T_407c0_row2_col2, #T_407c0_row2_col3, #T_407c0_row2_col4, #T_407c0_row2_col5, #T_407c0_row2_col6, #T_407c0_row2_col7, #T_407c0_row2_col8, #T_407c0_row2_col9, #T_407c0_row2_col10, #T_407c0_row2_col11, #T_407c0_row2_col12, #T_407c0_row2_col13, #T_407c0_row2_col14, #T_407c0_row2_col15, #T_407c0_row2_col16, #T_407c0_row2_col17, #T_407c0_row2_col18, #T_407c0_row2_col19, #T_407c0_row3_col0, #T_407c0_row3_col1, #T_407c0_row3_col2, #T_407c0_row3_col3, #T_407c0_row3_col4, #T_407c0_row3_col5, #T_407c0_row3_col6, #T_407c0_row3_col7, #T_407c0_row3_col8, #T_407c0_row3_col9, #T_407c0_row3_col10, #T_407c0_row3_col11, #T_407c0_row3_col12, #T_407c0_row3_col13, #T_407c0_row3_col14, #T_407c0_row3_col15, #T_407c0_row3_col16, #T_407c0_row3_col17, #T_407c0_row3_col18, #T_407c0_row3_col19, #T_407c0_row4_col0, #T_407c0_row4_col1, #T_407c0_row4_col2, #T_407c0_row4_col3, #T_407c0_row4_col4, #T_407c0_row4_col5, #T_407c0_row4_col6, #T_407c0_row4_col7, #T_407c0_row4_col8, #T_407c0_row4_col9, #T_407c0_row4_col10, #T_407c0_row4_col11, #T_407c0_row4_col12, #T_407c0_row4_col13, #T_407c0_row4_col14, #T_407c0_row4_col15, #T_407c0_row4_col16, #T_407c0_row4_col17, #T_407c0_row4_col18, #T_407c0_row4_col19, #T_407c0_row5_col0, #T_407c0_row5_col1, #T_407c0_row5_col2, #T_407c0_row5_col16, #T_407c0_row5_col17, #T_407c0_row5_col18, #T_407c0_row5_col19, #T_407c0_row6_col0, #T_407c0_row6_col1, #T_407c0_row6_col2, #T_407c0_row6_col19, #T_407c0_row7_col0, #T_407c0_row7_col1, #T_407c0_row7_col2, #T_407c0_row7_col19, #T_407c0_row8_col0, #T_407c0_row8_col1, #T_407c0_row8_col2, #T_407c0_row8_col8, #T_407c0_row8_col9, #T_407c0_row8_col10, #T_407c0_row8_col11, #T_407c0_row8_col12, #T_407c0_row8_col13, #T_407c0_row8_col19, #T_407c0_row9_col0, #T_407c0_row9_col1, #T_407c0_row9_col2, #T_407c0_row9_col3, #T_407c0_row9_col4, #T_407c0_row9_col5, #T_407c0_row9_col6, #T_407c0_row9_col7, #T_407c0_row9_col8, #T_407c0_row9_col9, #T_407c0_row9_col10, #T_407c0_row9_col11, #T_407c0_row9_col12, #T_407c0_row9_col13, #T_407c0_row9_col19 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #ffffff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_407c0_row5_col3, #T_407c0_row8_col14 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #f9f9f9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_407c0_row5_col4 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #b9b9b9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_407c0_row5_col5 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #c1c1c1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_407c0_row5_col6 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #858585;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row5_col7, #T_407c0_row5_col10, #T_407c0_row5_col11, #T_407c0_row5_col12, #T_407c0_row5_col13 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #777777;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row5_col8 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #090909;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row5_col9 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #5b5b5b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row5_col14 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #919191;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row5_col15 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #e1e1e1;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_407c0_row6_col3 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #727272;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row6_col4, #T_407c0_row6_col5, #T_407c0_row6_col6, #T_407c0_row6_col7, #T_407c0_row6_col8, #T_407c0_row6_col9, #T_407c0_row6_col10, #T_407c0_row6_col11, #T_407c0_row6_col12, #T_407c0_row6_col13, #T_407c0_row6_col14, #T_407c0_row7_col3, #T_407c0_row7_col4, #T_407c0_row7_col5, #T_407c0_row7_col6, #T_407c0_row7_col15, #T_407c0_row7_col16, #T_407c0_row7_col17, #T_407c0_row8_col16, #T_407c0_row8_col17, #T_407c0_row8_col18, #T_407c0_row9_col15, #T_407c0_row9_col16 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #000000;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row6_col15 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #020202;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row6_col16 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #363636;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row6_col17 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #9d9d9d;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row6_col18 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #dfdfdf;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_407c0_row7_col7 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #161616;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row7_col8, #T_407c0_row7_col9, #T_407c0_row7_col10, #T_407c0_row7_col11, #T_407c0_row7_col13 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #535353;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row7_col12 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #7c7c7c;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row7_col14 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #3d3d3d;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row7_col18 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #999999;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row8_col3 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #eaeaea;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_407c0_row8_col4 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #d0d0d0;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_407c0_row8_col5, #T_407c0_row8_col6 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #eeeeee;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_407c0_row8_col7 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #f3f3f3;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_407c0_row8_col15 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #232323;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row9_col14 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #c2c2c2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_407c0_row9_col17 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #080808;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_407c0_row9_col18 {\n",
              "  font-size: 6pt;\n",
              "  background-color: #c4c4c4;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_407c0_\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >0</th>\n",
              "      <th class=\"col_heading level0 col1\" >1</th>\n",
              "      <th class=\"col_heading level0 col2\" >2</th>\n",
              "      <th class=\"col_heading level0 col3\" >3</th>\n",
              "      <th class=\"col_heading level0 col4\" >4</th>\n",
              "      <th class=\"col_heading level0 col5\" >5</th>\n",
              "      <th class=\"col_heading level0 col6\" >6</th>\n",
              "      <th class=\"col_heading level0 col7\" >7</th>\n",
              "      <th class=\"col_heading level0 col8\" >8</th>\n",
              "      <th class=\"col_heading level0 col9\" >9</th>\n",
              "      <th class=\"col_heading level0 col10\" >10</th>\n",
              "      <th class=\"col_heading level0 col11\" >11</th>\n",
              "      <th class=\"col_heading level0 col12\" >12</th>\n",
              "      <th class=\"col_heading level0 col13\" >13</th>\n",
              "      <th class=\"col_heading level0 col14\" >14</th>\n",
              "      <th class=\"col_heading level0 col15\" >15</th>\n",
              "      <th class=\"col_heading level0 col16\" >16</th>\n",
              "      <th class=\"col_heading level0 col17\" >17</th>\n",
              "      <th class=\"col_heading level0 col18\" >18</th>\n",
              "      <th class=\"col_heading level0 col19\" >19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_407c0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_407c0_row0_col0\" class=\"data row0 col0\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col1\" class=\"data row0 col1\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col2\" class=\"data row0 col2\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col3\" class=\"data row0 col3\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col4\" class=\"data row0 col4\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col5\" class=\"data row0 col5\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col6\" class=\"data row0 col6\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col7\" class=\"data row0 col7\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col8\" class=\"data row0 col8\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col9\" class=\"data row0 col9\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col10\" class=\"data row0 col10\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col11\" class=\"data row0 col11\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col12\" class=\"data row0 col12\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col13\" class=\"data row0 col13\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col14\" class=\"data row0 col14\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col15\" class=\"data row0 col15\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col16\" class=\"data row0 col16\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col17\" class=\"data row0 col17\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col18\" class=\"data row0 col18\" >0</td>\n",
              "      <td id=\"T_407c0_row0_col19\" class=\"data row0 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_407c0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_407c0_row1_col0\" class=\"data row1 col0\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col1\" class=\"data row1 col1\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col2\" class=\"data row1 col2\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col3\" class=\"data row1 col3\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col4\" class=\"data row1 col4\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col5\" class=\"data row1 col5\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col6\" class=\"data row1 col6\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col7\" class=\"data row1 col7\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col8\" class=\"data row1 col8\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col9\" class=\"data row1 col9\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col10\" class=\"data row1 col10\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col11\" class=\"data row1 col11\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col12\" class=\"data row1 col12\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col13\" class=\"data row1 col13\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col14\" class=\"data row1 col14\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col15\" class=\"data row1 col15\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col16\" class=\"data row1 col16\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col17\" class=\"data row1 col17\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col18\" class=\"data row1 col18\" >0</td>\n",
              "      <td id=\"T_407c0_row1_col19\" class=\"data row1 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_407c0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_407c0_row2_col0\" class=\"data row2 col0\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col1\" class=\"data row2 col1\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col2\" class=\"data row2 col2\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col3\" class=\"data row2 col3\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col4\" class=\"data row2 col4\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col5\" class=\"data row2 col5\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col6\" class=\"data row2 col6\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col7\" class=\"data row2 col7\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col8\" class=\"data row2 col8\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col9\" class=\"data row2 col9\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col10\" class=\"data row2 col10\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col11\" class=\"data row2 col11\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col12\" class=\"data row2 col12\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col13\" class=\"data row2 col13\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col14\" class=\"data row2 col14\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col15\" class=\"data row2 col15\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col16\" class=\"data row2 col16\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col17\" class=\"data row2 col17\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col18\" class=\"data row2 col18\" >0</td>\n",
              "      <td id=\"T_407c0_row2_col19\" class=\"data row2 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_407c0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_407c0_row3_col0\" class=\"data row3 col0\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col1\" class=\"data row3 col1\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col2\" class=\"data row3 col2\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col3\" class=\"data row3 col3\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col4\" class=\"data row3 col4\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col5\" class=\"data row3 col5\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col6\" class=\"data row3 col6\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col7\" class=\"data row3 col7\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col8\" class=\"data row3 col8\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col9\" class=\"data row3 col9\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col10\" class=\"data row3 col10\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col11\" class=\"data row3 col11\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col12\" class=\"data row3 col12\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col13\" class=\"data row3 col13\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col14\" class=\"data row3 col14\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col15\" class=\"data row3 col15\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col16\" class=\"data row3 col16\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col17\" class=\"data row3 col17\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col18\" class=\"data row3 col18\" >0</td>\n",
              "      <td id=\"T_407c0_row3_col19\" class=\"data row3 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_407c0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_407c0_row4_col0\" class=\"data row4 col0\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col1\" class=\"data row4 col1\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col2\" class=\"data row4 col2\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col3\" class=\"data row4 col3\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col4\" class=\"data row4 col4\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col5\" class=\"data row4 col5\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col6\" class=\"data row4 col6\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col7\" class=\"data row4 col7\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col8\" class=\"data row4 col8\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col9\" class=\"data row4 col9\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col10\" class=\"data row4 col10\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col11\" class=\"data row4 col11\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col12\" class=\"data row4 col12\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col13\" class=\"data row4 col13\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col14\" class=\"data row4 col14\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col15\" class=\"data row4 col15\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col16\" class=\"data row4 col16\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col17\" class=\"data row4 col17\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col18\" class=\"data row4 col18\" >0</td>\n",
              "      <td id=\"T_407c0_row4_col19\" class=\"data row4 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_407c0_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_407c0_row5_col0\" class=\"data row5 col0\" >0</td>\n",
              "      <td id=\"T_407c0_row5_col1\" class=\"data row5 col1\" >0</td>\n",
              "      <td id=\"T_407c0_row5_col2\" class=\"data row5 col2\" >0</td>\n",
              "      <td id=\"T_407c0_row5_col3\" class=\"data row5 col3\" >12</td>\n",
              "      <td id=\"T_407c0_row5_col4\" class=\"data row5 col4\" >99</td>\n",
              "      <td id=\"T_407c0_row5_col5\" class=\"data row5 col5\" >91</td>\n",
              "      <td id=\"T_407c0_row5_col6\" class=\"data row5 col6\" >142</td>\n",
              "      <td id=\"T_407c0_row5_col7\" class=\"data row5 col7\" >155</td>\n",
              "      <td id=\"T_407c0_row5_col8\" class=\"data row5 col8\" >246</td>\n",
              "      <td id=\"T_407c0_row5_col9\" class=\"data row5 col9\" >182</td>\n",
              "      <td id=\"T_407c0_row5_col10\" class=\"data row5 col10\" >155</td>\n",
              "      <td id=\"T_407c0_row5_col11\" class=\"data row5 col11\" >155</td>\n",
              "      <td id=\"T_407c0_row5_col12\" class=\"data row5 col12\" >155</td>\n",
              "      <td id=\"T_407c0_row5_col13\" class=\"data row5 col13\" >155</td>\n",
              "      <td id=\"T_407c0_row5_col14\" class=\"data row5 col14\" >131</td>\n",
              "      <td id=\"T_407c0_row5_col15\" class=\"data row5 col15\" >52</td>\n",
              "      <td id=\"T_407c0_row5_col16\" class=\"data row5 col16\" >0</td>\n",
              "      <td id=\"T_407c0_row5_col17\" class=\"data row5 col17\" >0</td>\n",
              "      <td id=\"T_407c0_row5_col18\" class=\"data row5 col18\" >0</td>\n",
              "      <td id=\"T_407c0_row5_col19\" class=\"data row5 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_407c0_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_407c0_row6_col0\" class=\"data row6 col0\" >0</td>\n",
              "      <td id=\"T_407c0_row6_col1\" class=\"data row6 col1\" >0</td>\n",
              "      <td id=\"T_407c0_row6_col2\" class=\"data row6 col2\" >0</td>\n",
              "      <td id=\"T_407c0_row6_col3\" class=\"data row6 col3\" >138</td>\n",
              "      <td id=\"T_407c0_row6_col4\" class=\"data row6 col4\" >254</td>\n",
              "      <td id=\"T_407c0_row6_col5\" class=\"data row6 col5\" >254</td>\n",
              "      <td id=\"T_407c0_row6_col6\" class=\"data row6 col6\" >254</td>\n",
              "      <td id=\"T_407c0_row6_col7\" class=\"data row6 col7\" >254</td>\n",
              "      <td id=\"T_407c0_row6_col8\" class=\"data row6 col8\" >254</td>\n",
              "      <td id=\"T_407c0_row6_col9\" class=\"data row6 col9\" >254</td>\n",
              "      <td id=\"T_407c0_row6_col10\" class=\"data row6 col10\" >254</td>\n",
              "      <td id=\"T_407c0_row6_col11\" class=\"data row6 col11\" >254</td>\n",
              "      <td id=\"T_407c0_row6_col12\" class=\"data row6 col12\" >254</td>\n",
              "      <td id=\"T_407c0_row6_col13\" class=\"data row6 col13\" >254</td>\n",
              "      <td id=\"T_407c0_row6_col14\" class=\"data row6 col14\" >254</td>\n",
              "      <td id=\"T_407c0_row6_col15\" class=\"data row6 col15\" >252</td>\n",
              "      <td id=\"T_407c0_row6_col16\" class=\"data row6 col16\" >210</td>\n",
              "      <td id=\"T_407c0_row6_col17\" class=\"data row6 col17\" >122</td>\n",
              "      <td id=\"T_407c0_row6_col18\" class=\"data row6 col18\" >33</td>\n",
              "      <td id=\"T_407c0_row6_col19\" class=\"data row6 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_407c0_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_407c0_row7_col0\" class=\"data row7 col0\" >0</td>\n",
              "      <td id=\"T_407c0_row7_col1\" class=\"data row7 col1\" >0</td>\n",
              "      <td id=\"T_407c0_row7_col2\" class=\"data row7 col2\" >0</td>\n",
              "      <td id=\"T_407c0_row7_col3\" class=\"data row7 col3\" >220</td>\n",
              "      <td id=\"T_407c0_row7_col4\" class=\"data row7 col4\" >254</td>\n",
              "      <td id=\"T_407c0_row7_col5\" class=\"data row7 col5\" >254</td>\n",
              "      <td id=\"T_407c0_row7_col6\" class=\"data row7 col6\" >254</td>\n",
              "      <td id=\"T_407c0_row7_col7\" class=\"data row7 col7\" >235</td>\n",
              "      <td id=\"T_407c0_row7_col8\" class=\"data row7 col8\" >189</td>\n",
              "      <td id=\"T_407c0_row7_col9\" class=\"data row7 col9\" >189</td>\n",
              "      <td id=\"T_407c0_row7_col10\" class=\"data row7 col10\" >189</td>\n",
              "      <td id=\"T_407c0_row7_col11\" class=\"data row7 col11\" >189</td>\n",
              "      <td id=\"T_407c0_row7_col12\" class=\"data row7 col12\" >150</td>\n",
              "      <td id=\"T_407c0_row7_col13\" class=\"data row7 col13\" >189</td>\n",
              "      <td id=\"T_407c0_row7_col14\" class=\"data row7 col14\" >205</td>\n",
              "      <td id=\"T_407c0_row7_col15\" class=\"data row7 col15\" >254</td>\n",
              "      <td id=\"T_407c0_row7_col16\" class=\"data row7 col16\" >254</td>\n",
              "      <td id=\"T_407c0_row7_col17\" class=\"data row7 col17\" >254</td>\n",
              "      <td id=\"T_407c0_row7_col18\" class=\"data row7 col18\" >75</td>\n",
              "      <td id=\"T_407c0_row7_col19\" class=\"data row7 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_407c0_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_407c0_row8_col0\" class=\"data row8 col0\" >0</td>\n",
              "      <td id=\"T_407c0_row8_col1\" class=\"data row8 col1\" >0</td>\n",
              "      <td id=\"T_407c0_row8_col2\" class=\"data row8 col2\" >0</td>\n",
              "      <td id=\"T_407c0_row8_col3\" class=\"data row8 col3\" >35</td>\n",
              "      <td id=\"T_407c0_row8_col4\" class=\"data row8 col4\" >74</td>\n",
              "      <td id=\"T_407c0_row8_col5\" class=\"data row8 col5\" >35</td>\n",
              "      <td id=\"T_407c0_row8_col6\" class=\"data row8 col6\" >35</td>\n",
              "      <td id=\"T_407c0_row8_col7\" class=\"data row8 col7\" >25</td>\n",
              "      <td id=\"T_407c0_row8_col8\" class=\"data row8 col8\" >0</td>\n",
              "      <td id=\"T_407c0_row8_col9\" class=\"data row8 col9\" >0</td>\n",
              "      <td id=\"T_407c0_row8_col10\" class=\"data row8 col10\" >0</td>\n",
              "      <td id=\"T_407c0_row8_col11\" class=\"data row8 col11\" >0</td>\n",
              "      <td id=\"T_407c0_row8_col12\" class=\"data row8 col12\" >0</td>\n",
              "      <td id=\"T_407c0_row8_col13\" class=\"data row8 col13\" >0</td>\n",
              "      <td id=\"T_407c0_row8_col14\" class=\"data row8 col14\" >13</td>\n",
              "      <td id=\"T_407c0_row8_col15\" class=\"data row8 col15\" >224</td>\n",
              "      <td id=\"T_407c0_row8_col16\" class=\"data row8 col16\" >254</td>\n",
              "      <td id=\"T_407c0_row8_col17\" class=\"data row8 col17\" >254</td>\n",
              "      <td id=\"T_407c0_row8_col18\" class=\"data row8 col18\" >153</td>\n",
              "      <td id=\"T_407c0_row8_col19\" class=\"data row8 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_407c0_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_407c0_row9_col0\" class=\"data row9 col0\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col1\" class=\"data row9 col1\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col2\" class=\"data row9 col2\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col3\" class=\"data row9 col3\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col4\" class=\"data row9 col4\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col5\" class=\"data row9 col5\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col6\" class=\"data row9 col6\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col7\" class=\"data row9 col7\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col8\" class=\"data row9 col8\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col9\" class=\"data row9 col9\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col10\" class=\"data row9 col10\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col11\" class=\"data row9 col11\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col12\" class=\"data row9 col12\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col13\" class=\"data row9 col13\" >0</td>\n",
              "      <td id=\"T_407c0_row9_col14\" class=\"data row9 col14\" >90</td>\n",
              "      <td id=\"T_407c0_row9_col15\" class=\"data row9 col15\" >254</td>\n",
              "      <td id=\"T_407c0_row9_col16\" class=\"data row9 col16\" >254</td>\n",
              "      <td id=\"T_407c0_row9_col17\" class=\"data row9 col17\" >247</td>\n",
              "      <td id=\"T_407c0_row9_col18\" class=\"data row9 col18\" >53</td>\n",
              "      <td id=\"T_407c0_row9_col19\" class=\"data row9 col19\" >0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ],
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7f5f4199ae90>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "#hide_output\n",
        "df = pd.DataFrame(im3_t[:10,:20])\n",
        "df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1yoLLmhtHoK"
      },
      "source": [
        "<img alt=\"Top section of a digit\" width=\"490\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00059.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yic3UjctHoK"
      },
      "source": [
        "There's a top edge at cell 5,8. Let's repeat our calculation there:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "oX-UUbBNtHoK",
        "outputId": "c290d2a1-cc7a-4ca7-9858-80a6213075bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(762.)"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "(im3_t[4:7,6:9] * top_edge).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB6lCCkjtHoK"
      },
      "source": [
        "There's a right edge at cell 8,18. What does that give us?:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "XzNmrEG2tHoL",
        "outputId": "6ac0c44d-3989-4e27-bc3c-1c940f1463d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-29.)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "(im3_t[7:10,17:20] * top_edge).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVFrO6A0tHoL"
      },
      "source": [
        "As you can see, this little calculation is returning a high number where the 3×3-pixel square represents a top edge (i.e., where there are low values at the top of the square, and high values immediately underneath). That's because the `-1` values in our kernel have little impact in that case, but the `1` values have a lot.\n",
        "\n",
        "Let's look a tiny bit at the math. The filter will take any window of size 3×3 in our images, and if we name the pixel values like this:\n",
        "\n",
        "$$\\begin{matrix} a1 & a2 & a3 \\\\ a4 & a5 & a6 \\\\ a7 & a8 & a9 \\end{matrix}$$\n",
        "\n",
        "it will return $-a1-a2-a3+a7+a8+a9$. If we are in a part of the image where $a1$, $a2$, and $a3$ add up to the same as $a7$, $a8$, and $a9$, then the terms will cancel each other out and we will get 0. However, if $a7$ is greater than $a1$, $a8$ is greater than $a2$, and $a9$ is greater than $a3$, we will get a bigger number as a result. So this filter detects horizontal edges—more precisely, edges where we go from bright parts of the image at the top to darker parts at the bottom.\n",
        "\n",
        "Changing our filter to have the row of `1`s at the top and the `-1`s at the bottom would detect horizontal edges that go from dark to light. Putting the `1`s and `-1`s in columns versus rows would give us filters that detect vertical edges. Each set of weights will produce a different kind of outcome.\n",
        "\n",
        "Let's create a function to do this for one location, and check it matches our result from before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6HznFerPtHoM"
      },
      "outputs": [],
      "source": [
        "def apply_kernel(row, col, kernel):\n",
        "    return (im3_t[row-1:row+2,col-1:col+2] * kernel).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Di-h3SjntHoM",
        "outputId": "b0673247-a2d4-434e-f693-26d3e9573f3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(762.)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "apply_kernel(5,7,top_edge)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM3HLLOmtHoM"
      },
      "source": [
        "But note that we can't apply it to the corner (e.g., location 0,0), since there isn't a complete 3×3 square there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDUEnGI8tHoN"
      },
      "source": [
        "### Mapping a Convolution Kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eokLo2KtHoN"
      },
      "source": [
        "We can map `apply_kernel()` across the coordinate grid. That is, we'll be taking our 3×3 kernel, and applying it to each 3×3 section of our image. For instance, <<nopad_conv>> shows the positions a 3×3 kernel can be applied to in the first row of a 5×5 image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_vins7VtHoN"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter9_nopadconv.svg?raw=1\" id=\"nopad_conv\" caption=\"Applying a kernel across a grid\" alt=\"Applying a kernel across a grid\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWI1tdw4tHoN"
      },
      "source": [
        "To get a grid of coordinates we can use a *nested list comprehension*, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "TyzNQkeetHoN",
        "outputId": "14583566-c060-449f-d567-e043cd6b1531",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(1, 1), (1, 2), (1, 3), (1, 4)],\n",
              " [(2, 1), (2, 2), (2, 3), (2, 4)],\n",
              " [(3, 1), (3, 2), (3, 3), (3, 4)],\n",
              " [(4, 1), (4, 2), (4, 3), (4, 4)]]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "[[(i,j) for j in range(1,5)] for i in range(1,5)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk2bicw9tHoN"
      },
      "source": [
        "> note: Nested List Comprehensions: Nested list comprehensions are used a lot in Python, so if you haven't seen them before, take a few minutes to make sure you understand what's happening here, and experiment with writing your own nested list comprehensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf3cT0O_tHoO"
      },
      "source": [
        "Here's the result of applying our kernel over a coordinate grid:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "mvZEcWcHtHoO",
        "outputId": "247d51d8-5f21-4dcb-f4ea-8084772377d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "rng = range(1,27)\n",
        "top_edge3 = tensor([[apply_kernel(i,j,top_edge) for j in rng] for i in rng])\n",
        "\n",
        "show_image(top_edge3);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqNMmafRtHoO"
      },
      "source": [
        "Looking good! Our top edges are black, and bottom edges are white (since they are the *opposite* of top edges). Now that our image contains negative numbers too, `matplotlib` has automatically changed our colors so that white is the smallest number in the image, black the highest, and zeros appear as gray.\n",
        "\n",
        "We can try the same thing for left edges:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KpALpY_ctHoO",
        "outputId": "7c5b752a-d3c6-4592-c216-71519d4a3249",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 72x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "left_edge = tensor([[-1,1,0],\n",
        "                    [-1,1,0],\n",
        "                    [-1,1,0]]).float()\n",
        "\n",
        "left_edge3 = tensor([[apply_kernel(i,j,left_edge) for j in rng] for i in rng])\n",
        "\n",
        "show_image(left_edge3);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl1-WmW6tHoP"
      },
      "source": [
        "As we mentioned before, a convolution is the operation of applying such a kernel over a grid in this way. In the paper [\"A Guide to Convolution Arithmetic for Deep Learning\"](https://arxiv.org/abs/1603.07285) there are many great diagrams showing how image kernels can be applied. Here's an example from the paper showing (at the bottom) a light blue 4×4 image, with a dark blue 3×3 kernel being applied, creating a 2×2 green output activation map at the top. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czf1XjrYtHoP"
      },
      "source": [
        "<img alt=\"Result of applying a 3×3 kernel to a 4×4 image\" width=\"782\" caption=\"Result of applying a 3×3 kernel to a 4×4 image (courtesy of Vincent Dumoulin and Francesco Visin)\" id=\"three_ex_four_conv\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00028.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6A_OFBatHoP"
      },
      "source": [
        "Look at the shape of the result. If the original image has a height of `h` and a width of `w`, how many 3×3 windows can we find? As you can see from the example, there are `h-2` by `w-2` windows, so the image we get has a result as a height of `h-2` and a width of `w-2`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5NoGohGtHoP"
      },
      "source": [
        "We won't implement this convolution function from scratch, but use PyTorch's implementation instead (it is way faster than anything we could do in Python)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buFz6ir1tHoP"
      },
      "source": [
        "### Convolutions in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ7lKkU-tHoP"
      },
      "source": [
        "Convolution is such an important and widely used operation that PyTorch has it built in. It's called `F.conv2d` (recall that `F` is a fastai import from `torch.nn.functional`, as recommended by PyTorch). The PyTorch docs tell us that it includes these parameters:\n",
        "\n",
        "- input:: input tensor of shape `(minibatch, in_channels, iH, iW)`\n",
        "- weight:: filters of shape `(out_channels, in_channels, kH, kW)`\n",
        "\n",
        "Here `iH,iW` is the height and width of the image (i.e., `28,28`), and `kH,kW` is the height and width of our kernel (`3,3`). But apparently PyTorch is expecting rank-4 tensors for both these arguments, whereas currently we only have rank-2 tensors (i.e., matrices, or arrays with two axes).\n",
        "\n",
        "The reason for these extra axes is that PyTorch has a few tricks up its sleeve. The first trick is that PyTorch can apply a convolution to multiple images at the same time. That means we can call it on every item in a batch at once!\n",
        "\n",
        "The second trick is that PyTorch can apply multiple kernels at the same time. So let's create the diagonal-edge kernels too, and then stack all four of our edge kernels into a single tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "aduT8NontHoP",
        "outputId": "c1247da9-6e84-4bcf-9799-c9fc1f434e78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 3, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "diag1_edge = tensor([[ 0,-1, 1],\n",
        "                     [-1, 1, 0],\n",
        "                     [ 1, 0, 0]]).float()\n",
        "diag2_edge = tensor([[ 1,-1, 0],\n",
        "                     [ 0, 1,-1],\n",
        "                     [ 0, 0, 1]]).float()\n",
        "\n",
        "edge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge])\n",
        "edge_kernels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6tW5FL2tHoQ"
      },
      "source": [
        "To test this, we'll need a `DataLoader` and a sample mini-batch. Let's use the data block API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "PTeWt3jztHoQ",
        "outputId": "a402feed-61c3-4a35-e151-70e7130fe9cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-5375fed8d12d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                   get_y=parent_label)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/block.py\u001b[0m in \u001b[0;36mdataloaders\u001b[0;34m(self, source, path, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mdsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'verbose'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdsets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_item\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_tfms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_tfms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/block.py\u001b[0m in \u001b[0;36mdatasets\u001b[0;34m(self, source, verbose)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m                     \u001b[0;34m;\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Collecting items from {source}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_items\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m;\u001b[0m \u001b[0mpv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Found {len(items)} items\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitter\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mRandomSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mpv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{len(splits)} datasets of sizes {','.join([str(len(s)) for s in splits])}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/transforms.py\u001b[0m in \u001b[0;36mget_image_files\u001b[0;34m(path, recurse, folders)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_image_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;34m\"Get image files in `path` recursively, only in `folders`, if specified.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_extensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/transforms.py\u001b[0m in \u001b[0;36mget_files\u001b[0;34m(path, extensions, recurse, folders, followlinks)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m                         \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0m_get_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/transforms.py\u001b[0m in \u001b[0;36m_get_files\u001b[0;34m(p, fs, extensions)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     res = [p/f for f in fs if not f.startswith('.')\n\u001b[0m\u001b[1;32m     24\u001b[0m            and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/data/transforms.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     res = [p/f for f in fs if not f.startswith('.')\n\u001b[0;32m---> 24\u001b[0;31m            and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/pathlib.py\u001b[0m in \u001b[0;36m__truediv__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__truediv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rtruediv__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/pathlib.py\u001b[0m in \u001b[0;36m_make_child\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0mdrv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         drv, root, parts = self._flavour.join_parsed_parts(\n\u001b[1;32m    706\u001b[0m             self._drv, self._root, self._parts, drv, root, parts)\n",
            "\u001b[0;32m/usr/lib/python3.7/pathlib.py\u001b[0m in \u001b[0;36m_parse_args\u001b[0;34m(cls, args)\u001b[0m\n\u001b[1;32m    665\u001b[0m                         \u001b[0;34m\"object returning str, not %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                         % type(a))\n\u001b[0;32m--> 667\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/pathlib.py\u001b[0m in \u001b[0;36mparse_parts\u001b[0;34m(self, parts)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0maltsep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maltsep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mdrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "mnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), \n",
        "                  get_items=get_image_files, \n",
        "                  splitter=GrandparentSplitter(),\n",
        "                  get_y=parent_label)\n",
        "\n",
        "dls = mnist.dataloaders(path)\n",
        "xb,yb = first(dls.valid)\n",
        "xb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7yS1AgItHoQ"
      },
      "source": [
        "By default, fastai puts data on the GPU when using data blocks. Let's move it to the CPU for our examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYB4dQqGtHoQ"
      },
      "outputs": [],
      "source": [
        "xb,yb = to_cpu(xb),to_cpu(yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc8qVC3itHoQ"
      },
      "source": [
        "One batch contains 64 images, each of 1 channel, with 28×28 pixels. `F.conv2d` can handle multichannel (i.e., color) images too. A *channel* is a single basic color in an image—for regular full-color images there are three channels, red, green, and blue. PyTorch represents an image as a rank-3 tensor, with dimensions `[channels, rows, columns]`.\n",
        "\n",
        "We'll see how to handle more than one channel later in this chapter. Kernels passed to `F.conv2d` need to be rank-4 tensors: `[channels_in, features_out, rows, columns]`. `edge_kernels` is currently missing one of these. We need to tell PyTorch that the number of input channels in the kernel is one, which we can do by inserting an axis of size one (this is known as a *unit axis*) in the first location, where the PyTorch docs show `in_channels` is expected. To insert a unit axis into a tensor, we use the `unsqueeze` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzztLtQEtHoQ"
      },
      "outputs": [],
      "source": [
        "edge_kernels.shape,edge_kernels.unsqueeze(1).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buUss5CFtHoR"
      },
      "source": [
        "This is now the correct shape for `edge_kernels`. Let's pass this all to `conv2d`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_0y0wVPtHoR"
      },
      "outputs": [],
      "source": [
        "edge_kernels = edge_kernels.unsqueeze(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqi_JHBwtHoR"
      },
      "outputs": [],
      "source": [
        "batch_features = F.conv2d(xb, edge_kernels)\n",
        "batch_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbPfvaZntHoR"
      },
      "source": [
        "The output shape shows we gave 64 images in the mini-batch, 4 kernels, and 26×26 edge maps (we started with 28×28 images, but lost one pixel from each side as discussed earlier). We can see we get the same results as when we did this manually:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKOaSffktHoR"
      },
      "outputs": [],
      "source": [
        "show_image(batch_features[0,0]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgcRAWI4tHoS"
      },
      "source": [
        "The most important trick that PyTorch has up its sleeve is that it can use the GPU to do all this work in parallel—that is, applying multiple kernels, to multiple images, across multiple channels. Doing lots of work in parallel is critical to getting GPUs to work efficiently; if we did each of these operations one at a time, we'd often run hundreds of times slower (and if we used our manual convolution loop from the previous section, we'd be millions of times slower!). Therefore, to become a strong deep learning practitioner, one skill to practice is giving your GPU plenty of work to do at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDpIL-GytHoS"
      },
      "source": [
        "It would be nice to not lose those two pixels on each axis. The way we do that is to add *padding*, which is simply additional pixels added around the outside of our image. Most commonly, pixels of zeros are added. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MOrIqEEtHoS"
      },
      "source": [
        "### Strides and Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrf_CEGbtHoS"
      },
      "source": [
        "With appropriate padding, we can ensure that the output activation map is the same size as the original image, which can make things a lot simpler when we construct our architectures. <<pad_conv>> shows how adding padding allows us to apply the kernels in the image corners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8eeKjyCtHoS"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter9_padconv.svg?raw=1\" id=\"pad_conv\" caption=\"A convolution with padding\" alt=\"A convolution with padding\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YBnO8C3tHoS"
      },
      "source": [
        "With a 5×5 input, 4×4 kernel, and 2 pixels of padding, we end up with a 6×6 activation map, as we can see in <<four_by_five_conv>>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxBmeV7OtHoS"
      },
      "source": [
        "<img alt=\"A 4×4 kernel with 5×5 input and 2 pixels of padding\" width=\"783\" caption=\"A 4×4 kernel with 5×5 input and 2 pixels of padding (courtesy of Vincent Dumoulin and Francesco Visin)\" id=\"four_by_five_conv\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00029.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PskKKoItHoS"
      },
      "source": [
        "If we add a kernel of size `ks` by `ks` (with `ks` an odd number), the necessary padding on each side to keep the same shape is `ks//2`. An even number for `ks` would require a different amount of padding on the top/bottom and left/right, but in practice we almost never use an even filter size.\n",
        "\n",
        "So far, when we have applied the kernel to the grid, we have moved it one pixel over at a time. But we can jump further; for instance, we could move over two pixels after each kernel application, as in <<three_by_five_conv>>. This is known as a *stride-2* convolution. The most common kernel size in practice is 3×3, and the most common padding is 1. As you'll see, stride-2 convolutions are useful for decreasing the size of our outputs, and stride-1 convolutions are useful for adding layers without changing the output size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlbppssjtHoT"
      },
      "source": [
        "<img alt=\"A 3×3 kernel with 5×5 input, stride-2 convolution, and 1 pixel of padding\" width=\"774\" caption=\"A 3×3 kernel with 5×5 input, stride-2 convolution, and 1 pixel of padding (courtesy of Vincent Dumoulin and Francesco Visin)\" id=\"three_by_five_conv\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00030.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFY860FMtHoT"
      },
      "source": [
        "In an image of size `h` by `w`, using a padding of 1 and a stride of 2 will give us a result of size `(h+1)//2` by `(w+1)//2`. The general formula for each dimension is `(n + 2*pad - ks)//stride + 1`, where `pad` is the padding, `ks`, the size of our kernel, and `stride` is the stride."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hadVryr_tHoT"
      },
      "source": [
        "Let's now take a look at how the pixel values of the result of our convolutions are computed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mnKjodwtHoT"
      },
      "source": [
        "### Understanding the Convolution Equations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z4YN5iltHoT"
      },
      "source": [
        "To explain the math behind convolutions, fast.ai student Matt Kleinsmith came up with the very clever idea of showing [CNNs from different viewpoints](https://medium.com/impactai/cnns-from-different-viewpoints-fab7f52d159c). In fact, it's so clever, and so helpful, we're going to show it here too!\n",
        "\n",
        "Here's our 3×3 pixel image, with each pixel labeled with a letter:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRFaFGH4tHoT"
      },
      "source": [
        "<img alt=\"The image\" width=\"75\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00032.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30d3W0m-tHoT"
      },
      "source": [
        "And here's our kernel, with each weight labeled with a Greek letter:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIHvyBx1tHoT"
      },
      "source": [
        "<img alt=\"The kernel\" width=\"55\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00033.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK0j8MtPtHoX"
      },
      "source": [
        "Since the filter fits in the image four times, we have four results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKumlCzHtHoX"
      },
      "source": [
        "<img alt=\"The activations\" width=\"52\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00034.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuoGQotktHoX"
      },
      "source": [
        "<<apply_kernel>> shows how we applied the kernel to each section of the image to yield each result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YjPc9-ztHoX"
      },
      "source": [
        "<img alt=\"Applying the kernel\" width=\"366\" caption=\"Applying the kernel\" id=\"apply_kernel\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00035.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp6Iw3PftHoX"
      },
      "source": [
        "The equation view is in <<eq_view>>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "380XrNBmtHoX"
      },
      "source": [
        "<img alt=\"The equation\" width=\"436\" caption=\"The equation\" id=\"eq_view\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00036.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV9kxhQ-tHoX"
      },
      "source": [
        "Notice that the bias term, *b*, is the same for each section of the image. You can consider the bias as part of the filter, just like the weights (α, β, γ, δ) are part of the filter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63jUD4OUtHoY"
      },
      "source": [
        "Here's an interesting insight—a convolution can be represented as a special kind of matrix multiplication, as illustrated in <<conv_matmul>>. The weight matrix is just like the ones from traditional neural networks. However, this weight matrix has two special properties:\n",
        "\n",
        "1. The zeros shown in gray are untrainable. This means that they’ll stay zero throughout the optimization process.\n",
        "1. Some of the weights are equal, and while they are trainable (i.e., changeable), they must remain equal. These are called *shared weights*.\n",
        "\n",
        "The zeros correspond to the pixels that the filter can't touch. Each row of the weight matrix corresponds to one application of the filter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jImvBGNvtHoY"
      },
      "source": [
        "<img alt=\"Convolution as matrix multiplication\" width=\"683\" caption=\"Convolution as matrix multiplication\" id=\"conv_matmul\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00038.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUeq1fITtHoY"
      },
      "source": [
        "Now that we understand what a convolution is, let's use them to build a neural net."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_SJkhbStHoY"
      },
      "source": [
        "## Our First Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I0ttzuUtHoY"
      },
      "source": [
        "There is no reason to believe that some particular edge filters are the most useful kernels for image recognition. Furthermore, we've seen that in later layers convolutional kernels become complex transformations of features from lower levels, but we don't have a good idea of how to manually construct these.\n",
        "\n",
        "Instead, it would be best to learn the values of the kernels. We already know how to do this—SGD! In effect, the model will learn the features that are useful for classification.\n",
        "\n",
        "When we use convolutions instead of (or in addition to) regular linear layers we create a *convolutional neural network* (CNN)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7tMS6bVtHoY"
      },
      "source": [
        "### Creating the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV_GSmzetHoY"
      },
      "source": [
        "Let's go back to the  basic neural network we had in <<chapter_mnist_basics>>. It was defined like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nb7kPY5tHoY"
      },
      "outputs": [],
      "source": [
        "simple_net = nn.Sequential(\n",
        "    nn.Linear(28*28,30),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(30,1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ4WBu4vtHoY"
      },
      "source": [
        "We can view a model's definition:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJLwQyWMtHoZ"
      },
      "outputs": [],
      "source": [
        "simple_net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IsSbfVPtHoZ"
      },
      "source": [
        "We now want to create a similar architecture to this linear model, but using convolutional layers instead of linear. `nn.Conv2d` is the module equivalent of `F.conv2d`. It's more convenient than `F.conv2d` when creating an architecture, because it creates the weight matrix for us automatically when we instantiate it.\n",
        "\n",
        "Here's a possible architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X45YVLFQtHoZ"
      },
      "outputs": [],
      "source": [
        "broken_cnn = sequential(\n",
        "    nn.Conv2d(1,30, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.Conv2d(30,1, kernel_size=3, padding=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlcL_SZhtHoZ"
      },
      "source": [
        "One thing to note here is that we didn't need to specify 28×28 as the input size. That's because a linear layer needs a weight in the weight matrix for every pixel, so it needs to know how many pixels there are, but a convolution is applied over each pixel automatically. The weights only depend on the number of input and output channels and the kernel size, as we saw in the previous section.\n",
        "\n",
        "Think about what the output shape is going to be, then let's try it and see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK8btLkLtHoZ"
      },
      "outputs": [],
      "source": [
        "broken_cnn(xb).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcbJakKktHoZ"
      },
      "source": [
        "This is not something we can use to do classification, since we need a single output activation per image, not a 28×28 map of activations. One way to deal with this is to use enough stride-2 convolutions such that the final layer is size 1. That is, after one stride-2 convolution the size will be 14×14, after two it will be 7×7, then 4×4, 2×2, and finally size 1.\n",
        "\n",
        "Let's try that now. First, we'll define a function with the basic parameters we'll use in each convolution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKwaVfl0tHoZ"
      },
      "outputs": [],
      "source": [
        "def conv(ni, nf, ks=3, act=True):\n",
        "    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n",
        "    if act: res = nn.Sequential(res, nn.ReLU())\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLrSZzJ7tHoa"
      },
      "source": [
        "> important: Refactoring: Refactoring parts of your neural networks like this makes it much less likely you'll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Rrg0DPtHoa"
      },
      "source": [
        "When we use a stride-2 convolution, we often increase the number of features at the same time. This is because we're decreasing the number of activations in the activation map by a factor of 4; we don't want to decrease the capacity of a layer by too much at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCrH10o6tHoa"
      },
      "source": [
        "> jargon: channels and features: These two terms are largely used interchangeably, and refer to the size of the second axis of a weight matrix, which is, the number of activations per grid cell after a convolution. _Features_ is never used to refer to the input data, but _channels_ can refer to either the input data (generally channels are colors) or activations inside the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCHBUi6jtHoa"
      },
      "source": [
        "Here is how we can build a simple CNN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayYo-hi3tHoa"
      },
      "outputs": [],
      "source": [
        "simple_cnn = sequential(\n",
        "    conv(1 ,4),            #14x14\n",
        "    conv(4 ,8),            #7x7\n",
        "    conv(8 ,16),           #4x4\n",
        "    conv(16,32),           #2x2\n",
        "    conv(32,2, act=False), #1x1\n",
        "    Flatten(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEl1F2bftHoa"
      },
      "source": [
        "> j: I like to add comments like the ones here after each convolution to show how large the activation map will be after each layer. These comments assume that the input size is 28*28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcom8ListHoa"
      },
      "source": [
        "Now the network outputs two activations, which map to the two possible levels in our labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Lx7OOqetHoa"
      },
      "outputs": [],
      "source": [
        "simple_cnn(xb).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSLSsg65tHob"
      },
      "source": [
        "We can now create our `Learner`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzeH0N5atHob"
      },
      "outputs": [],
      "source": [
        "learn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYNHt-4ytHob"
      },
      "source": [
        "To see exactly what's going on in the model, we can use `summary`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNMNnghxtHob"
      },
      "outputs": [],
      "source": [
        "learn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5sCjxqutHob"
      },
      "source": [
        "Note that the output of the final `Conv2d` layer is `64x2x1x1`. We need to remove those extra `1x1` axes; that's what `Flatten` does. It's basically the same as PyTorch's `squeeze` method, but as a module.\n",
        "\n",
        "Let's see if this trains! Since this is a deeper network than we've built from scratch before, we'll use a lower learning rate and more epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyV2-A6FtHob"
      },
      "outputs": [],
      "source": [
        "learn.fit_one_cycle(2, 0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Llu2Jf7ZtHob"
      },
      "source": [
        "Success! It's getting closer to the `resnet18` result we had, although it's not quite there yet, and it's taking more epochs, and we're needing to use a lower learning rate. We still have a few more tricks to learn, but we're getting closer and closer to being able to create a modern CNN from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4I3B3rqtHob"
      },
      "source": [
        "### Understanding Convolution Arithmetic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e7hcQfNtHoc"
      },
      "source": [
        "We can see from the summary that we have an input of size `64x1x28x28`. The axes are `batch,channel,height,width`. This is often represented as `NCHW` (where `N` refers to batch size). Tensorflow, on the other hand, uses `NHWC` axis order. The first layer is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsqT4Vy0tHoc"
      },
      "outputs": [],
      "source": [
        "m = learn.model[0]\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixR_9XHRtHoc"
      },
      "source": [
        "So we have 1 input channel, 4 output channels, and a 3×3 kernel. Let's check the weights of the first convolution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1iASwJQtHod"
      },
      "outputs": [],
      "source": [
        "m[0].weight.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85wqqWeztHod"
      },
      "source": [
        "The summary shows we have 40 parameters, and `4*1*3*3` is 36. What are the other four parameters? Let's see what the bias contains:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_XywC2-tHod"
      },
      "outputs": [],
      "source": [
        "m[0].bias.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLX3Hb-otHod"
      },
      "source": [
        "We can now use this information to clarify our statement in the previous section: \"When we use a stride-2 convolution, we often increase the number of features because we're decreasing the number of activations in the activation map by a factor of 4; we don't want to decrease the capacity of a layer by too much at a time.\"\n",
        "\n",
        "There is one bias for each channel. (Sometimes channels are called *features* or *filters* when they are not input channels.) The output shape is `64x4x14x14`, and this will therefore become the input shape to the next layer. The next layer, according to `summary`, has 296 parameters. Let's ignore the batch axis to keep things simple. So for each of `14*14=196` locations we are multiplying `296-8=288` weights (ignoring the bias for simplicity), so that's `196*288=56_448` multiplications at this layer. The next layer will have `7*7*(1168-16)=56_448` multiplications.\n",
        "\n",
        "What happened here is that our stride-2 convolution halved the *grid size* from `14x14` to `7x7`, and we doubled the *number of filters* from 8 to 16, resulting in no overall change in the amount of computation. If we left the number of channels the same in each stride-2 layer, the amount of computation being done in the net would get less and less as it gets deeper. But we know that the deeper layers have to compute semantically rich features (such as eyes or fur), so we wouldn't expect that doing *less* computation would make sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaudatFNtHoe"
      },
      "source": [
        "Another way to think of this is based on receptive fields."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49J3bcxutHoe"
      },
      "source": [
        "### Receptive Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns0tFa2dtHoe"
      },
      "source": [
        "The *receptive field* is the area of an image that is involved in the calculation of a layer. On the [book's website](https://book.fast.ai/), you'll find an Excel spreadsheet called *conv-example.xlsx* that shows the calculation of two stride-2 convolutional layers using an MNIST digit. Each layer has a single kernel. <<preced1>> shows what we see if we click on one of the cells in the *conv2* section, which shows the output of the second convolutional layer, and click *trace precedents*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgHJjLDDtHoe"
      },
      "source": [
        "<img alt=\"Immediate precedents of conv2 layer\" width=\"308\" caption=\"Immediate precedents of Conv2 layer\" id=\"preced1\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00068.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFvNMWzctHoe"
      },
      "source": [
        "Here, the cell with the green border is the cell we clicked on, and the blue highlighted cells are its *precedents*—that is, the cells used to calculate its value. These cells are the corresponding 3×3 area of cells from the input layer (on the left), and the cells from the filter (on the right). Let's now click *trace precedents* again, to see what cells are used to calculate these inputs. <<preced2>> shows what happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1xrFy2UtHoe"
      },
      "source": [
        "<img alt=\"Secondary precedents of conv2 layer\" width=\"601\" caption=\"Secondary precedents of Conv2 layer\" id=\"preced2\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00069.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H9RVY__tHof"
      },
      "source": [
        "In this example, we have just two convolutional layers, each of stride 2, so this is now tracing right back to the input image. We can see that a 7×7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This 7×7 area is the *receptive field* in the input of the green activation in Conv2. We can also see that a second filter kernel is needed now, since we have two layers.\n",
        "\n",
        "As you see from this example, the deeper we are in the network (specifically, the more stride-2 convs we have before a layer), the larger the receptive field for an activation in that layer. A large receptive field means that a large amount of the input image is used to calculate each activation in that layer is. We now know that in the deeper layers of the network we have semantically rich features, corresponding to larger receptive fields. Therefore, we'd expect that we'd need more weights for each of our features to handle this increasing complexity. This is another way of saying the same thing we mentioned in the previous section: when we introduce a stride-2 conv in our network, we should also increase the number of channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy_V8jHdtHof"
      },
      "source": [
        "When writing this particular chapter, we had a lot of questions we needed answers for, to be able to explain CNNs to you as best we could. Believe it or not, we found most of the answers on Twitter. We're going to take a quick break to talk to you about that now, before we move on to color images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A538XSoqtHof"
      },
      "source": [
        "### A Note About Twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFwVeXN1tHof"
      },
      "source": [
        "We are not, to say the least, big users of social networks in general. But our goal in writing this book is to help you become the best deep learning practitioner you can, and we would be remiss not to mention how important Twitter has been in our own deep learning journeys.\n",
        "\n",
        "You see, there's another part of Twitter, far away from Donald Trump and the Kardashians, which is the part of Twitter where deep learning researchers and practitioners talk shop every day. As we were writing this section, Jeremy wanted to double-check that what we were saying about stride-2 convolutions was accurate, so he asked on Twitter:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E1MIU6ltHof"
      },
      "source": [
        "<img alt=\"twitter 1\" width=\"500\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00064.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Z_Ez1utHog"
      },
      "source": [
        "A few minutes later, this answer popped up:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1Xm7a9itHog"
      },
      "source": [
        "<img alt=\"twitter 2\" width=\"500\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00065.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lGALtEntHog"
      },
      "source": [
        "Christian Szegedy is the first author of [Inception](https://arxiv.org/pdf/1409.4842.pdf), the 2014 ImageNet winner and source of many key insights used in modern neural networks. Two hours later, this appeared:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4c0u0SdtHog"
      },
      "source": [
        "<img alt=\"twitter 3\" width=\"500\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00066.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KLvPTZStHog"
      },
      "source": [
        "Do you recognize that name? You saw it in <<chapter_production>>, when we were talking about the Turing Award winners who established the foundations of deep learning today!\n",
        "\n",
        "Jeremy also asked on Twitter for help checking our description of label smoothing in <<chapter_sizing_and_tta>> was accurate, and got a response again from directly from Christian Szegedy (label smoothing was originally introduced in the Inception paper):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTK4VHRJtHog"
      },
      "source": [
        "<img alt=\"twitter 4\" width=\"500\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00067.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9iS1edxtHoh"
      },
      "source": [
        "Many of the top people in deep learning today are Twitter regulars, and are very open about interacting with the wider community. One good way to get started is to look at a list of Jeremy's [recent Twitter likes](https://twitter.com/jeremyphoward/likes), or [Sylvain's](https://twitter.com/GuggerSylvain/likes). That way, you can see a list of Twitter users that we think have interesting and useful things to say.\n",
        "\n",
        "Twitter is the main way we both stay up to date with interesting papers, software releases, and other deep learning news. For making connections with the deep learning community, we recommend getting involved both in the [fast.ai forums](https://forums.fast.ai) and on Twitter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txcPFrkFtHoh"
      },
      "source": [
        "That said, let's get back to the meat of this chapter. Up until now, we have only shown you examples of pictures in black and white, with one value per pixel. In practice, most colored images have three values per pixel to define their color. We'll look at working with color images next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EZSJ7_AtHoh"
      },
      "source": [
        "## Color Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx_nftPCtHoh"
      },
      "source": [
        "A colour picture is a rank-3 tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI6XXufftHoh"
      },
      "outputs": [],
      "source": [
        "im = image2tensor(Image.open(image_bear()))\n",
        "im.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dtk8M2IJtHoh"
      },
      "outputs": [],
      "source": [
        "show_image(im);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2YwPXdVtHoh"
      },
      "source": [
        "The first axis contains the channels, red, green, and blue:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T07bzXL3tHoh"
      },
      "outputs": [],
      "source": [
        "_,axs = subplots(1,3)\n",
        "for bear,ax,color in zip(im,axs,('Reds','Greens','Blues')):\n",
        "    show_image(255-bear, ax=ax, cmap=color)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv4lfY7XtHoi"
      },
      "source": [
        "We saw what the convolution operation was for one filter on one channel of the image (our examples were done on a square). A convolutional layer will take an image with a certain number of channels (three for the first layer for regular RGB color images) and output an image with a different number of channels. Like our hidden size that represented the numbers of neurons in a linear layer, we can decide to have as many filters as we want, and each of them will be able to specialize, some to detect horizontal edges, others to detect vertical edges and so forth, to give something like we studied in <<chapter_production>>.\n",
        "\n",
        "In one sliding window, we have a certain number of channels and we need as many filters (we don't use the same kernel for all the channels). So our kernel doesn't have a size of 3 by 3, but `ch_in` (for channels in) is 3 by 3. On each channel, we multiply the elements of our window by the elements of the coresponding filter, then sum the results (as we saw before) and sum over all the filters. In the example given in <<rgbconv>>, the result of our conv layer on that window is red + green + blue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frtY4oPVtHoi"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter9_rgbconv.svg?raw=1\" id=\"rgbconv\" caption=\"Convolution over an RGB image\" alt=\"Convolution over an RGB image\" width=\"550\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-LTi60BtHoi"
      },
      "source": [
        "So, in order to apply a convolution to a color picture we require a kernel tensor with a size that matches the first axis. At each location, the corresponding parts of the kernel and the image patch are multiplied together.\n",
        "\n",
        "These are then all added together, to produce a single number, for each grid location, for each output feature, as shown in <<rgbconv2>>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE3Jw5dQtHoi"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/chapter9_rgb_conv_stack.svg?raw=1\" id=\"rgbconv2\" caption=\"Adding the RGB filters\" alt=\"Adding the RGB filters\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayxZPTYdtHoi"
      },
      "source": [
        "Then we have `ch_out` filters like this, so in the end, the result of our convolutional layer will be a batch of images with `ch_out` channels and a height and width given by the formula outlined earlier. This give us `ch_out` tensors of size `ch_in x ks x ks` that we represent in one big tensor of four dimensions. In PyTorch, the order of the dimensions for those weights is `ch_out x ch_in x ks x ks`.\n",
        "\n",
        "Additionally, we may want to have a bias for each filter. In the preceding example, the final result for our convolutional layer would be $y_{R} + y_{G} + y_{B} + b$ in that case. Like in a linear layer, there are as many bias as we have kernels, so the biases is a vector of size `ch_out`.\n",
        "\n",
        "There are no special mechanisms required when setting up a CNN for training with color images. Just make sure your first layer has three inputs.\n",
        "\n",
        "There are lots of ways of processing color images. For instance, you can change them to black and white, change from RGB to HSV (hue, saturation, and value) color space, and so forth. In general, it turns out experimentally that changing the encoding of colors won't make any difference to your model results, as long as you don't lose information in the transformation. So, transforming to black and white is a bad idea, since it removes the color information entirely (and this can be critical; for instance, a pet breed may have a distinctive color); but converting to HSV generally won't make any difference.\n",
        "\n",
        "Now you know what those pictures in <<chapter_intro>> of \"what a neural net learns\" from the [Zeiler and Fergus paper](https://arxiv.org/abs/1311.2901) mean! This is their picture of some of the layer 1 weights which we showed:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juh2APhqtHoi"
      },
      "source": [
        "<img alt=\"Layer 1 kernels found by Zeiler and Fergus\" width=\"120\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00031.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBqtJHRDtHoi"
      },
      "source": [
        "This is taking the three slices of the convolutional kernel, for each output feature, and displaying them as images. We can see that even though the creators of the neural net never explicitly created kernels to find edges, for instance, the neural net automatically discovered these features using SGD.\n",
        "\n",
        "Now let's see how we can train these CNNs, and show you all the techniques fastai uses under the hood for efficient training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw1Lq3XQtHoi"
      },
      "source": [
        "## Improving Training Stability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df2fGP9DtHoj"
      },
      "source": [
        "Since we are so good at recognizing 3s from 7s, let's move on to something harder—recognizing all 10 digits. That means we'll need to use `MNIST` instead of `MNIST_SAMPLE`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEaz18uYtHoj"
      },
      "outputs": [],
      "source": [
        "path = untar_data(URLs.MNIST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa02CCJftHoj"
      },
      "outputs": [],
      "source": [
        "#hide\n",
        "Path.BASE_PATH = path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcHdz-_HtHoj"
      },
      "outputs": [],
      "source": [
        "path.ls()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8IQ5LDftHok"
      },
      "source": [
        "The data is in two folders named *training* and *testing*, so we have to tell `GrandparentSplitter` about that (it defaults to `train` and `valid`). We did do that in the `get_dls` function, which we create to make it easy to change our batch size later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YyPEu70tHok"
      },
      "outputs": [],
      "source": [
        "def get_dls(bs=64):\n",
        "    return DataBlock(\n",
        "        blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), \n",
        "        get_items=get_image_files, \n",
        "        splitter=GrandparentSplitter('training','testing'),\n",
        "        get_y=parent_label,\n",
        "        batch_tfms=Normalize()\n",
        "    ).dataloaders(path, bs=bs)\n",
        "\n",
        "dls = get_dls()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq46gpb-tHok"
      },
      "source": [
        "Remember, it's always a good idea to look at your data before you use it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3zMy61PtHok"
      },
      "outputs": [],
      "source": [
        "dls.show_batch(max_n=9, figsize=(4,4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmKPKNxUtHok"
      },
      "source": [
        "Now that we have our data ready, we can train a simple model on it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlzaoviRtHok"
      },
      "source": [
        "### A Simple Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb1aKOv-tHok"
      },
      "source": [
        "Earlier in this chapter, we built a model based on a `conv` function like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i97-XxoctHok"
      },
      "outputs": [],
      "source": [
        "def conv(ni, nf, ks=3, act=True):\n",
        "    res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)\n",
        "    if act: res = nn.Sequential(res, nn.ReLU())\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlP6TtI-tHol"
      },
      "source": [
        "Let's start with a basic CNN as a baseline. We'll use the same one as earlier, but with one tweak: we'll use more activations. Since we have more numbers to differentiate, it's likely we will need to learn more filters.\n",
        "\n",
        "As we discussed, we generally want to double the number of filters each time we have a stride-2 layer. One way to increase the number of filters throughout our network is to double the number of activations in the first layer–then every layer after that will end up twice as big as in the previous version as well.\n",
        "\n",
        "But there is a subtle problem with this. Consider the kernel that is being applied to each pixel. By default, we use a 3×3-pixel kernel. That means that there are a total of 3×3 = 9 pixels that the kernel is being applied to at each location. Previously, our first layer had four output filters. That meant that there were four values being computed from nine pixels at each location. Think about what happens if we double this output to eight filters. Then when we apply our kernel we will be using nine pixels to calculate eight numbers. That means it isn't really learning much at all: the output size is almost the same as the input size. Neural networks will only create useful features if they're forced to do so—that is, if the number of outputs from an operation is significantly smaller than the number of inputs.\n",
        "\n",
        "To fix this, we can use a larger kernel in the first layer. If we use a kernel of 5×5 pixels then there are 25 pixels being used at each kernel application. Creating eight filters from this will mean the neural net will have to find some useful features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKNhDWfttHol"
      },
      "outputs": [],
      "source": [
        "def simple_cnn():\n",
        "    return sequential(\n",
        "        conv(1 ,8, ks=5),        #14x14\n",
        "        conv(8 ,16),             #7x7\n",
        "        conv(16,32),             #4x4\n",
        "        conv(32,64),             #2x2\n",
        "        conv(64,10, act=False),  #1x1\n",
        "        Flatten(),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_fz8fUvtHol"
      },
      "source": [
        "As you'll see in a moment, we can look inside our models while they're training in order to try to find ways to make them train better. To do this we use the `ActivationStats` callback, which records the mean, standard deviation, and histogram of activations of every trainable layer (as we've seen, callbacks are used to add behavior to the training loop; we'll explore how they work in <<chapter_accel_sgd>>):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGxxHPavtHol"
      },
      "outputs": [],
      "source": [
        "from fastai.callback.hook import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33K2YNjWtHol"
      },
      "source": [
        "We want to train quickly, so that means training at a high learning rate. Let's see how we go at 0.06:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Sj9BYVHtHol"
      },
      "outputs": [],
      "source": [
        "def fit(epochs=1):\n",
        "    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,\n",
        "                    metrics=accuracy, cbs=ActivationStats(with_hist=True))\n",
        "    learn.fit(epochs, 0.06)\n",
        "    return learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRVEQi-itHol"
      },
      "outputs": [],
      "source": [
        "learn = fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ7qjD3utHol"
      },
      "source": [
        "This didn't train at all well! Let's find out why.\n",
        "\n",
        "One handy feature of the callbacks passed to `Learner` is that they are made available automatically, with the same name as the callback class, except in `snake_case`. So, our `ActivationStats` callback can be accessed through `activation_stats`. I'm sure you remember `learn.recorder`... can you guess how that is implemented? That's right, it's a callback called `Recorder`!\n",
        "\n",
        "`ActivationStats` includes some handy utilities for plotting the activations during training. `plot_layer_stats(idx)` plots the mean and standard deviation of the activations of layer number *`idx`*, along with the percentage of activations near zero. Here's the first layer's plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tG9EGGytHom"
      },
      "outputs": [],
      "source": [
        "learn.activation_stats.plot_layer_stats(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4cKwVPOtHom"
      },
      "source": [
        "Generally our model should have a consistent, or at least smooth, mean and standard deviation of layer activations during training. Activations near zero are particularly problematic, because it means we have computation in the model that's doing nothing at all (since multiplying by zero gives zero). When you have some zeros in one layer, they will therefore generally carry over to the next layer... which will then create more zeros. Here's the penultimate layer of our network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAd3I78KtHom"
      },
      "outputs": [],
      "source": [
        "learn.activation_stats.plot_layer_stats(-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or8QPCDStHom"
      },
      "source": [
        "As expected, the problems get worse towards the end of the network, as the instability and zero activations compound over layers. Let's look at what we can do to make training more stable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT9yjSAvtHom"
      },
      "source": [
        "### Increase Batch Size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78KhZS97tHom"
      },
      "source": [
        "One way to make training more stable is to increase the batch size. Larger batches have gradients that are more accurate, since they're calculated from more data. On the downside, though, a larger batch size means fewer batches per epoch, which means less opportunities for your model to update weights. Let's see if a batch size of 512 helps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S-Eo-O8tHom"
      },
      "outputs": [],
      "source": [
        "dls = get_dls(512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pl5lJt84tHom"
      },
      "outputs": [],
      "source": [
        "learn = fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49GnIwP-tHon"
      },
      "source": [
        "Let's see what the penultimate layer looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE3GvJkVtHon"
      },
      "outputs": [],
      "source": [
        "learn.activation_stats.plot_layer_stats(-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SXokdtytHon"
      },
      "source": [
        "Again, we've got most of our activations near zero. Let's see what else we can do to improve training stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMzq-s_XtHon"
      },
      "source": [
        "### 1cycle Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCBD-ew3tHon"
      },
      "source": [
        "Our initial weights are not well suited to the task we're trying to solve. Therefore, it is dangerous to begin training with a high learning rate: we may very well make the training diverge instantly, as we've seen. We probably don't want to end training with a high learning rate either, so that we don't skip over a minimum. But we want to train at a high learning rate for the rest of the training period, because we'll be able to train more quickly that way. Therefore, we should change the learning rate during training, from low, to high, and then back to low again.\n",
        "\n",
        "Leslie Smith (yes, the same guy that invented the learning rate finder!) developed this idea in his article [\"Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates\"](https://arxiv.org/abs/1708.07120). He designed a schedule for learning rate separated into two phases: one where the learning rate grows from the minimum value to the maximum value (*warmup*), and one where it decreases back to the minimum value (*annealing*). Smith called this combination of approaches *1cycle training*.\n",
        "\n",
        "1cycle training allows us to use a much higher maximum learning rate than other types of training, which gives two benefits:\n",
        "\n",
        "- By training with higher learning rates, we train faster—a phenomenon Smith named *super-convergence*.\n",
        "- By training with higher learning rates, we overfit less because we skip over the sharp local minima to end up in a smoother (and therefore more generalizable) part of the loss.\n",
        "\n",
        "The second point is an interesting and subtle one; it is based on the observation that a model that generalizes well is one whose loss would not change very much if you changed the input by a small amount. If a model trains at a large learning rate for quite a while, and can find a good loss when doing so, it must have found an area that also generalizes well, because it is jumping around a lot from batch to batch (that is basically the definition of a high learning rate). The problem is that, as we have discussed, just jumping to a high learning rate is more likely to result in diverging losses, rather than seeing your losses improve. So we don't jump straight to a high learning rate. Instead, we start at a low learning rate, where our losses do not diverge, and we allow the optimizer to gradually find smoother and smoother areas of our parameters by gradually going to higher and higher learning rates.\n",
        "\n",
        "Then, once we have found a nice smooth area for our parameters, we want to find the very best part of that area, which means we have to bring our learning rates down again. This is why 1cycle training has a gradual learning rate warmup, and a gradual learning rate cooldown. Many researchers have found that in practice this approach leads to more accurate models and trains more quickly. That is why it is the approach that is used by default for `fine_tune` in fastai.\n",
        "\n",
        "In <<chapter_accel_sgd>> we'll learn all about *momentum* in SGD. Briefly, momentum is a technique where the optimizer takes a step not only in the direction of the gradients, but also that continues in the direction of previous steps. Leslie Smith introduced the idea of *cyclical momentums* in [\"A Disciplined Approach to Neural Network Hyper-Parameters: Part 1\"](https://arxiv.org/pdf/1803.09820.pdf). It suggests that the momentum varies in the opposite direction of the learning rate: when we are at high learning rates, we use less momentum, and we use more again in the annealing phase.\n",
        "\n",
        "We can use 1cycle training in fastai by calling `fit_one_cycle`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNvxYg-KtHon"
      },
      "outputs": [],
      "source": [
        "def fit(epochs=1, lr=0.06):\n",
        "    learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy,\n",
        "                    metrics=accuracy, cbs=ActivationStats(with_hist=True))\n",
        "    learn.fit_one_cycle(epochs, lr)\n",
        "    return learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgVg4CYztHon"
      },
      "outputs": [],
      "source": [
        "learn = fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jTgMONxtHon"
      },
      "source": [
        "We're finally making some progress! It's giving us a reasonable accuracy now.\n",
        "\n",
        "We can view the learning rate and momentum throughout training by calling `plot_sched` on `learn.recorder`. `learn.recorder` (as the name suggests) records everything that happens during training, including losses, metrics, and hyperparameters such as learning rate and momentum:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjUb7jt5tHoo"
      },
      "outputs": [],
      "source": [
        "learn.recorder.plot_sched()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NunxfFhtHoo"
      },
      "source": [
        "Smith's original 1cycle paper used a linear warmup and linear annealing. As you can see, we adapted the approach in fastai by combining it with another popular approach: cosine annealing. `fit_one_cycle` provides the following parameters you can adjust:\n",
        "\n",
        "- `lr_max`:: The highest learning rate that will be used (this can also be a list of learning rates for each layer group, or a Python `slice` object containing the first and last layer group learning rates)\n",
        "- `div`:: How much to divide `lr_max` by to get the starting learning rate\n",
        "- `div_final`::  How much to divide `lr_max` by to get the ending learning rate\n",
        "- `pct_start`:: What percentage of the batches to use for the warmup\n",
        "- `moms`:: A tuple `(mom1,mom2,mom3)` where *`mom1`* is the initial momentum, *`mom2`* is the minimum momentum, and *`mom3`* is the final momentum\n",
        "\n",
        "Let's take a look at our layer stats again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHUNsGPBtHoo"
      },
      "outputs": [],
      "source": [
        "learn.activation_stats.plot_layer_stats(-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HQnMzAetHoo"
      },
      "source": [
        "The percentage of near-zero weights is getting much better, although it's still quite high.\n",
        "\n",
        "We can see even more about what's going on in our training using `color_dim`, passing it a layer index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ienYEvy3tHoo"
      },
      "outputs": [],
      "source": [
        "learn.activation_stats.color_dim(-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6JV_-JNtHoo"
      },
      "source": [
        "`color_dim` was developed by fast.ai in conjunction with a student, Stefano Giomo. Stefano, who refers to the idea as the *colorful dimension*, provides an [in-depth explanation](https://forums.fast.ai/t/the-colorful-dimension/42908) of the history and details behind the method. The basic idea is to create a histogram of the activations of a layer, which we would hope would follow a smooth pattern such as the normal distribution (colorful_dist)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUeM6EU1tHop"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/colorful_dist.jpeg?raw=1\" id=\"colorful_dist\" caption=\"Histogram in 'colorful dimension'\" alt=\"Histogram in 'colorful dimension'\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQG8KtK4tHop"
      },
      "source": [
        "To create `color_dim`, we take the histogram shown on the left here, and convert it into just the colored representation shown at the bottom. Then we flip it on its side, as shown on the right. We found that the distribution is clearer if we take the log of the histogram values. Then, Stefano describes:\n",
        "\n",
        "> : The final plot for each layer is made by stacking the histogram of the activations from each batch along the horizontal axis. So each vertical slice in the visualisation represents the histogram of activations for a single batch. The color intensity corresponds to the height of the histogram, in other words the number of activations in each histogram bin.\n",
        "\n",
        "<<colorful_summ>> shows how this all fits together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEm5_LvutHop"
      },
      "source": [
        "<img src=\"https://github.com/fastai/fastbook/blob/master/images/colorful_summ.png?raw=1\" id=\"colorful_summ\" caption=\"Summary of the colorful dimension (courtesy of Stefano Giomo)\" alt=\"Summary of the colorful dimension\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktTN8dK9tHop"
      },
      "source": [
        "This illustrates why log(f) is more colorful than *f* when *f* follows a normal distribution because taking a log changes the Gaussian in a quadratic, which isn't as narrow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqIdzluWtHop"
      },
      "source": [
        "So with that in mind, let's take another look at the result for the penultimate layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2TmeRWrtHoq"
      },
      "outputs": [],
      "source": [
        "learn.activation_stats.color_dim(-2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJaNuK59tHoq"
      },
      "source": [
        "This shows a classic picture of \"bad training.\" We start with nearly all activations at zero—that's what we see at the far left, with all the dark blue. The bright yellow at the bottom represents the near-zero activations. Then, over the first few batches we see the number of nonzero activations exponentially increasing. But it goes too far, and collapses! We see the dark blue return, and the bottom becomes bright yellow again. It almost looks like training restarts from scratch. Then we see the activations increase again, and collapse again. After repeating this a few times, eventually we see a spread of activations throughout the range.\n",
        "\n",
        "It's much better if training can be smooth from the start. The cycles of exponential increase and then collapse tend to result in a lot of near-zero activations, resulting in slow training and poor final results. One way to solve this problem is to use batch normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsOSpm9GtHoq"
      },
      "source": [
        "### Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sp65Wi1gtHoq"
      },
      "source": [
        "To fix the slow training and poor final results we ended up with in the previous section, we need to fix the initial large percentage of near-zero activations, and then try to maintain a good distribution of activations throughout training.\n",
        "\n",
        "Sergey Ioffe and Christian Szegedy presented a solution to this problem in the 2015 paper [\"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\"](https://arxiv.org/abs/1502.03167). In the abstract, they describe just the problem that we've seen:\n",
        "\n",
        "> : Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization... We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.\n",
        "\n",
        "Their solution, they say is:\n",
        "\n",
        "> : Making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization.\n",
        "\n",
        "The paper caused great excitement as soon as it was released, because it included the chart in <<batchnorm>>, which clearly demonstrated that batch normalization could train a model that was even more accurate than the current state of the art (the *Inception* architecture) and around 5x faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PwwEuIAtHoq"
      },
      "source": [
        "<img alt=\"Impact of batch normalization\" width=\"553\" caption=\"Impact of batch normalization (courtesy of Sergey Ioffe and Christian Szegedy)\" id=\"batchnorm\" src=\"https://github.com/fastai/fastbook/blob/master/images/att_00046.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYlvaFo5tHoq"
      },
      "source": [
        "Batch normalization (often just called *batchnorm*) works by taking an average of the mean and standard deviations of the activations of a layer and using those to normalize the activations. However, this can cause problems because the network might want some activations to be really high in order to make accurate predictions. So they also added two learnable parameters (meaning they will be updated in the SGD step), usually called `gamma` and `beta`. After normalizing the activations to get some new activation vector `y`, a batchnorm layer returns `gamma*y + beta`.\n",
        "\n",
        "That's why our activations can have any mean or variance, independent from the mean and standard deviation of the results of the previous layer. Those statistics are learned separately, making training easier on our model. The behavior is different during training and validation: during training, we use the mean and standard deviation of the batch to normalize the data, while during validation we instead use a running mean of the statistics calculated during training.\n",
        "\n",
        "Let's add a batchnorm layer to `conv`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf6IV_kxtHor"
      },
      "outputs": [],
      "source": [
        "def conv(ni, nf, ks=3, act=True):\n",
        "    layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)]\n",
        "    if act: layers.append(nn.ReLU())\n",
        "    layers.append(nn.BatchNorm2d(nf))\n",
        "    return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPVVvrPxtHor"
      },
      "source": [
        "and fit our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6KP_8pEtHor"
      },
      "outputs": [],
      "source": [
        "learn = fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yv3ywOOtHor"
      },
      "source": [
        "That's a great result! Let's take a look at `color_dim`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMwDlfjRtHor"
      },
      "outputs": [],
      "source": [
        "learn.activation_stats.color_dim(-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWxQGe6FtHor"
      },
      "source": [
        "This is just what we hope to see: a smooth development of activations, with no \"crashes.\" Batchnorm has really delivered on its promise here! In fact, batchnorm has been so successful that we see it (or something very similar) in nearly all modern neural networks.\n",
        "\n",
        "An interesting observation about models containing batch normalization layers is that they tend to generalize better than models that don't contain them. Although we haven't as yet seen a rigorous analysis of what's going on here, most researchers believe that the reason for this is that batch normalization adds some extra randomness to the training process. Each mini-batch will have a somewhat different mean and standard deviation than other mini-batches. Therefore, the activations will be normalized by different values each time. In order for the model to make accurate predictions, it will have to learn to become robust to these variations. In general, adding additional randomization to the training process often helps.\n",
        "\n",
        "Since things are going so well, let's train for a few more epochs and see how it goes. In fact, let's *increase* the learning rate, since the abstract of the batchnorm paper claimed we should be able to \"train at much higher learning rates\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZR7cHuFtHor"
      },
      "outputs": [],
      "source": [
        "learn = fit(5, lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqL_UWYgtHor"
      },
      "source": [
        "At this point, I think it's fair to say we know how to recognize digits! It's time to move on to something harder..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUW_RBsTtHor"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxAJtHIwtHos"
      },
      "source": [
        "We've seen that convolutions are just a type of matrix multiplication, with two constraints on the weight matrix: some elements are always zero, and some elements are tied (forced to always have the same value). In <<chapter_intro>> we saw the eight requirements from the 1986 book *Parallel Distributed Processing*; one of them was \"A pattern of connectivity among units.\" That's exactly what these constraints do: they enforce a certain pattern of connectivity.\n",
        "\n",
        "These constraints allow us to use far fewer parameters in our model, without sacrificing the ability to represent complex visual features. That means we can train deeper models faster, with less overfitting. Although the universal approximation theorem shows that it should be *possible* to represent anything in a fully connected network in one hidden layer, we've seen now that in *practice* we can train much better models by being thoughtful about network architecture.\n",
        "\n",
        "Convolutions are by far the most common pattern of connectivity we see in neural nets (along with regular linear layers, which we refer to as *fully connected*), but it's likely that many more will be discovered.\n",
        "\n",
        "We've also seen how to interpret the activations of layers in the network to see whether training is going well or not, and how batchnorm helps regularize the training and makes it smoother. In the next chapter, we will use both of those layers to build the most popular architecture in computer vision: a residual network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKJPxCTStHos"
      },
      "source": [
        "## Questionnaire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVSs6XXbtHos"
      },
      "source": [
        "1. What is a \"feature\"?\n",
        "1. Write out the convolutional kernel matrix for a top edge detector.\n",
        "1. Write out the mathematical operation applied by a 3×3 kernel to a single pixel in an image.\n",
        "1. What is the value of a convolutional kernel apply to a 3×3 matrix of zeros?\n",
        "1. What is \"padding\"?\n",
        "1. What is \"stride\"?\n",
        "1. Create a nested list comprehension to complete any task that you choose.\n",
        "1. What are the shapes of the `input` and `weight` parameters to PyTorch's 2D convolution?\n",
        "1. What is a \"channel\"?\n",
        "1. What is the relationship between a convolution and a matrix multiplication?\n",
        "1. What is a \"convolutional neural network\"?\n",
        "1. What is the benefit of refactoring parts of your neural network definition?\n",
        "1. What is `Flatten`? Where does it need to be included in the MNIST CNN? Why?\n",
        "1. What does \"NCHW\" mean?\n",
        "1. Why does the third layer of the MNIST CNN have `7*7*(1168-16)` multiplications?\n",
        "1. What is a \"receptive field\"?\n",
        "1. What is the size of the receptive field of an activation after two stride 2 convolutions? Why?\n",
        "1. Run *conv-example.xlsx* yourself and experiment with *trace precedents*.\n",
        "1. Have a look at Jeremy or Sylvain's list of recent Twitter \"like\"s, and see if you find any interesting resources or ideas there.\n",
        "1. How is a color image represented as a tensor?\n",
        "1. How does a convolution work with a color input?\n",
        "1. What method can we use to see that data in `DataLoaders`?\n",
        "1. Why do we double the number of filters after each stride-2 conv?\n",
        "1. Why do we use a larger kernel in the first conv with MNIST (with `simple_cnn`)?\n",
        "1. What information does `ActivationStats` save for each layer?\n",
        "1. How can we access a learner's callback after training?\n",
        "1. What are the three statistics plotted by `plot_layer_stats`? What does the x-axis represent?\n",
        "1. Why are activations near zero problematic?\n",
        "1. What are the upsides and downsides of training with a larger batch size?\n",
        "1. Why should we avoid using a high learning rate at the start of training?\n",
        "1. What is 1cycle training?\n",
        "1. What are the benefits of training with a high learning rate?\n",
        "1. Why do we want to use a low learning rate at the end of training?\n",
        "1. What is \"cyclical momentum\"?\n",
        "1. What callback tracks hyperparameter values during training (along with other information)?\n",
        "1. What does one column of pixels in the `color_dim` plot represent?\n",
        "1. What does \"bad training\" look like in `color_dim`? Why?\n",
        "1. What trainable parameters does a batch normalization layer contain?\n",
        "1. What statistics are used to normalize in batch normalization during training? How about during validation?\n",
        "1. Why do models with batch normalization layers generalize better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Ri5wN8tHos"
      },
      "source": [
        "### Further Research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88aHOVHXtHos"
      },
      "source": [
        "1. What features other than edge detectors have been used in computer vision (especially before deep learning became popular)?\n",
        "1. There are other normalization layers available in PyTorch. Try them out and see what works best. Learn about why other normalization layers have been developed, and how they differ from batch normalization.\n",
        "1. Try moving the activation function after the batch normalization layer in `conv`. Does it make a difference? See what you can find out about what order is recommended, and why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54hfVpSKtHos"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "13_convolutions.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}